---
title: "homework"
author: "å”å† ç¾¤"
date: "2024-12-08"
output: html_document
---

# ç¬¬0æ¬¡ä½œä¸š

##ä¾‹1

```{r }
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
library(knitr)
kable(head(mtcars))
plot(mtcars$mpg, mtcars$wt, main = "Miles Per Gallon vs Weight",
     xlab = "Miles Per Gallon", ylab = "Weight")
plot(pressure)
```

##ä¾‹2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#æ²³å—çœå„å¸‚åç§°
city = c("éƒ‘å·","å¼€å°","æ´›é˜³","å¹³é¡¶å±±","å®‰é˜³","é¹¤å£","æ–°ä¹¡","ç„¦ä½œ","æ¿®é˜³","è®¸æ˜Œ","æ¼¯æ²³","ä¸‰é—¨å³¡","å—é˜³","å•†ä¸˜","ä¿¡é˜³","å‘¨å£","é©»é©¬åº—")
#æ²³å—çœå„å¸‚GDP
gdp <- array(c(3384,603,1441,677,605,259,806,562,421,904,451,416,1060,756,664,817,770),dim=(17))
#æ²³å—çœå„å¸‚äººå£
population <- array(c(12600574,4824016,7056699,4987137,5477614,1565973,6251929,3521078,3772088,4379998,2367490,2034872,9713112,7816831,6234401,9026015,7008427),dim=(17))
#æ„å»ºæ•°æ®æ¡†æ¶
data <- data.frame(city, gdp, population)
#å°†æ•°æ®ç”¨è¡¨æ ¼è¡¨ç¤º
knitr::kable(data, caption = "æ²³å—çœ2023å¹´å„å¸‚åŒºgdpä¸äººå£")


```

##ä¾‹3

```{r }
knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½æ‰€éœ€çš„åŒ…
library(ggplot2)

# ç¤ºä¾‹æ•°æ®ï¼šçˆ¶äº²ä¸å„¿å­çš„èº«é«˜
data <- data.frame(
  çˆ¶äº²èº«é«˜ = c(65, 66, 67, 68, 69, 70, 71, 72, 73, 74),  # çˆ¶äº²èº«é«˜æ•°æ®
  å„¿å­èº«é«˜ = c(63, 65, 66, 68, 69, 70, 71, 72, 73, 74)   # å„¿å­èº«é«˜æ•°æ®
)

# åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹
model <- lm(å„¿å­èº«é«˜ ~ çˆ¶äº²èº«é«˜, data = data)

# ç»˜åˆ¶æ•£ç‚¹å›¾å¹¶æ·»åŠ æ‹Ÿåˆçš„ç›´çº¿
ggplot(data, aes(x = çˆ¶äº²èº«é«˜, y = å„¿å­èº«é«˜)) +
  geom_point(color = "blue", size = 3) +          # ç»˜åˆ¶æ•£ç‚¹å›¾
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # æ·»åŠ æ‹Ÿåˆç›´çº¿
  labs(title = "çˆ¶äº²ä¸å„¿å­èº«é«˜çš„çº¿æ€§å›å½’", x = "çˆ¶äº²èº«é«˜ (è‹±å¯¸)", y = "å„¿å­èº«é«˜ (è‹±å¯¸)") +
  theme_minimal()
```


#ä¸Šè¿°çº¿æ€§å›å½’æ¨¡å‹ç”¨åˆ°çš„å…¬å¼
$$
y = \beta_0 + \beta_1 x + \epsilon
$$

# ç¬¬ä¸€æ¬¡ä½œä¸š

## Question1: 

The Rayleigh density [156, Ch. 18] is 
$$
f(x; \sigma) = \frac{x}{\sigma^2} e^{-\frac{x^2}{2\sigma^2}} \quad \text{for} \ x \geq 0,\quad\sigma \geq 0
$$ 
Develop an algorithm to generate random samples from a
Rayleigh($\sigma$) distribution. Generate Rayleigh($\sigma$) samples for
several choices of $\sigma \geq 0$ and check that the mode of the
generated samples is close to the theoretical mode $\sigma$ (check the
histogram).

## Answer1

### Idea

å…ˆé€šè¿‡å¯¹ç‘åˆ©åˆ†å¸ƒå¯†åº¦å‡½æ•°ç§¯åˆ†å¾—åˆ°ç‘åˆ©åˆ†å¸ƒçš„åˆ†å¸ƒå‡½æ•° 
$$
F(x; \sigma) = \int_0^x \frac{t}{\sigma^2} e^{-\frac{t^2}{2\sigma^2}} \, dt
= 1 - e^{-\frac{x^2}{2\sigma^2}}
$$
ä¹‹åä½¿ç”¨é€†å˜æ¢æ³•æŠŠç”Ÿæˆçš„0åˆ°1ä¸Šçš„å‡åŒ€åˆ†å¸ƒè½¬æ¢åˆ°ç‘åˆ©åˆ†å¸ƒä¸Šã€‚å–$U\sim[0,1]$ï¼Œä»¤$U = F_x(x)$,äºæ˜¯é€šè¿‡é€†å˜æ¢å¾—$$
X = \sqrt{-2\sigma^2 \ln(1 - U)}
$$
æœ€åé€šè¿‡å¯¹æ¯”å¾—åˆ°æ•°æ®çš„ç›´æ–¹å›¾ä¸åˆ†å¸ƒå‡½æ•°çš„æ›²çº¿æ¥éªŒè¯è¯•éªŒç»“æœçš„æ­£ç¡®æ€§ã€‚

### Code

```{r }
knitr::opts_chunk$set(echo = TRUE)
set.seed(156)
sig_list <- c(0.5,1,5,10,15)  #sigmaçš„å–å€¼ç»„
ylim_list <- c(2, 1, 0.2,0.1,0.1) #ç›´æ–¹å›¾ä¸­yçš„å–å€¼èŒƒå›´ç»„
k <- 1
for (sigma in sig_list){
  

u <- runif(1000)
x = sqrt(-2*sigma^2*log(1-u))
hist(x, prob=TRUE, main = expression(f(x) == (x/sigma^2)*e^(-x^2/(2*sigma^2))), ylim = c(0,ylim_list[k]))
y <- seq(0,50, .01)

lines(y, (y/sigma^2)*exp(-y^2/(2*sigma^2)))
k <- k+1
}
```

## Question2:
 Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have N(0,1) and N(3,1) distributions with mixing probabilities p1 and p2 =1âˆ’ p1. Graph the histogram of the sample with density superimposed, for p1 =0.75. Repeat with different values for p1 and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of p1 that produce bimodal mixtures.

## Answer2:

### Idea
è‹¥$X_1\sim N(0,1)$,$X_2\sim N(3,1)$,ä»¤$Y=p_1*X_1+p_2*X_2, \quadå…¶ä¸­p_1+p_2=1$ã€‚$X_1$ä¸$X_2$çš„å–å€¼å¯ä»¥ç›´æ¥ç”Ÿæˆï¼Œå¯¹äº$p_1$çš„ç”Ÿæˆï¼Œå¯ä»¥ç”Ÿæˆæœä»0-1å‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬ï¼Œå¯¹å…¶å’Œç•Œé™å€¼è¿›è¡Œæ¯”è¾ƒï¼Œæ¥çœ‹$p_1$æ˜¯0è¿˜æ˜¯1

### Code

```{r }
knitr::opts_chunk$set(echo = TRUE)
X1 <- rnorm(n = 1000, mean = 0, sd = 1)
X2 <- rnorm(n = 1000, mean = 3, sd = 1)
y <- seq(0,1, .1) #p1çš„å–å€¼ä»0åˆ°1ï¼Œæ¯éš”0.1å–ä¸€æ¬¡
standard <- c(0.75, y) #æ ¹æ®é¢˜ç›®è¦æ±‚æ·»åŠ påœ¨0.75æ—¶çš„å€¼
for(p1_judge in standard){
  u <- runif(1000)
  p1 <- ifelse(u > p1_judge, 0, 1)
  p2 <- 1-p1
  Y <- p1*X1 + p2*X2
  hist(Y, prob=TRUE, main = substitute(Y == p1 * X[1] +  p2 * X[2], list(p1 = p1_judge,p2 = 1-p1_judge)))
}
```

## Conclusion
å¯ä»¥å‘ç°ï¼Œåœ¨$p_1$=0.3æ—¶å¼€å§‹å‡ºç°åŒå³°æ•ˆåº”ï¼Œä¸€ç›´åˆ°$p_1$=0.7åŒå³°æ•ˆåº”å¼€å§‹ä¸æ˜¾è‘—


## Question3:
A compound Poisson process is a stochastic process \(\{X(t), t\geq 0\}\) that can be represented as the random sum \(X(t)=\sum_{i=1}^{N(t)} Y_{i}, t\geq 0\), where \(\{N(t), t\geq 0\}\) is a Poisson process and \(Y_1, Y_2, \ldots\) are iid and independent of \(\{N(t), t\geq 0\}\). Write a program to simulate a compound Poisson(Î»)-Gamma process (Y has a Gamma distribution). Estimate the mean and the variance of \(X(10)\) for several choices of the parameters and compare with the theoretical values. Hint: Show that \(E[X(t)]=\lambda t E[Y_1]\) and \(Var(X(t))=\lambda t E[Y_2]\).
 
## Answer2:

### Idea
ç”Ÿæˆä¸€äº›åˆ—å‚æ•°ä¸åŒçš„æ³Šæ¾åˆ†å¸ƒå’ŒGammaåˆ†å¸ƒï¼ŒæŒ‰ç…§å…¬å¼è®¡ç®—æœŸæœ›å’Œæ–¹å·®ï¼Œæœ€åå’Œæ ‡å‡†å€¼è¿›è¡Œå¯¹æ¯”

### Code
```{r}
library(ggplot2)
library(MASS)
knitr::opts_chunk$set(echo = TRUE)

# å®šä¹‰å˜é‡å
lambda_vals <- c(1, 2, 5, 8, 10)
alpha_vals <- c(2, 2, 2, 5, 5)
beta_vals <- c(1, 1, 2, 2, 5)
t <- 10  # å›ºå®šæ—¶é—´ç‚¹ t = 10
n_simulations <- 1000  # æ¨¡æ‹Ÿæ¬¡æ•°

# éå†æ¯ç»„å‚æ•°
for(k in 1:length(lambda_vals)){
  X10_values <- numeric(n_simulations)  # å­˜å‚¨æ¯æ¬¡æ¨¡æ‹Ÿçš„ X(10)
  
  # è¿›è¡Œå¤šæ¬¡æ¨¡æ‹Ÿ
  for(i in 1:n_simulations){
    # æ¨¡æ‹Ÿæ³Šæ¾è¿‡ç¨‹ N(t)
    N <- rpois(1, lambda_vals[k] * t)
    
    # æ¨¡æ‹Ÿ Gamma åˆ†å¸ƒéšæœºå˜é‡ Y_i
    Y <- rgamma(N, shape = alpha_vals[k], scale = 1 / beta_vals[k])
    
    # è®¡ç®— X(10) å¹¶å­˜å‚¨
    X10_values[i] <- sum(Y)
  }
  
  # è®¡ç®—æ¨¡æ‹Ÿå¾—åˆ°çš„å‡å€¼å’Œæ–¹å·®
  mean_simulated <- mean(X10_values)
  var_simulated <- var(X10_values)
  
  # è®¡ç®—ç†è®ºå€¼
  E_Y <- alpha_vals[k] / beta_vals[k]  # Gamma åˆ†å¸ƒçš„æœŸæœ›
  Var_Y <- alpha_vals[k] / beta_vals[k]^2  # Gamma åˆ†å¸ƒçš„æ–¹å·®
  
  E_X10 <- lambda_vals[k] * t * E_Y  # X(t) çš„ç†è®ºæœŸæœ›
  Var_X10 <- lambda_vals[k] * t * (Var_Y + E_Y^2)  # X(t) çš„ç†è®ºæ–¹å·®
  
  # æ‰“å°ç»“æœ
  cat("å‚æ•°ç»„åˆ", k, ": lambda =", lambda_vals[k], ", alpha =", alpha_vals[k], ", beta =", beta_vals[k], "\n")
  cat("æ¨¡æ‹Ÿå¾—åˆ°çš„å‡å€¼:", mean_simulated, "\n")
  cat("ç†è®ºå‡å€¼:", E_X10, "\n")
  cat("æ¨¡æ‹Ÿå¾—åˆ°çš„æ–¹å·®:", var_simulated, "\n")
  cat("ç†è®ºæ–¹å·®:", Var_X10, "\n\n")
}



```

# ç¬¬äºŒæ¬¡ä½œä¸š

# Problem1
 
 Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf,
 and use the function to estimate F(x) forx =0.1,0.2,...,0.9. Compare the
 estimates with the values returned by the pbeta function in R
 
# Answer1

## Idea

è€ƒè™‘æŠŠè¿™ä¸ªç§¯åˆ†çœ‹æˆæŸä¸ªå…³äºå‡åŒ€åˆ†å¸ƒå˜é‡å‡½æ•°Xçš„æœŸæœ›ï¼Œç„¶åæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œç”¨æ ·æœ¬å‡å€¼æ¨¡æ‹ŸæœŸæœ›
$$
\int_0^t x^\alpha (1 - x)^\beta/B(\alpha, \beta) dx = \int_0^t t x^\alpha (1 - x)^\beta \cdot \frac{1}{t*B(\alpha, \beta)} dx = t \mathbb{E}[x^\alpha (1 - x)^\beta]/B(\alpha, \beta)
$$

## Code


```{r }
knitr::opts_chunk$set(echo = TRUE)

m <- 1e4
alpha <- 3
beta <- 3
t_list <- seq(0.1, 1, by = 0.1)
for( t in t_list){
  x <- runif(m, min = 0, max = t)
  F_x <- t * x^(alpha-1) * (1-x)^(beta-1) /beta(alpha,beta)
  F_estimate = mean(F_x) 
  F_theoretical = pbeta(t, alpha, beta)
  cat("tä¸º", t,"æ—¶,  ç†è®ºå€¼ä¸º:",F_theoretical,"   å®é™…å€¼ä¸º:", F_estimate ,"\n")
  
}
```


# Problem2

5.9 The Rayleigh density [156, (18.76)] is
$$
f(x) = \frac{x}{\sigma^2} e^{-\frac{x^2}{2\sigma^2}}, \quad x \geq 0, \sigma > 0.
$$

Implement a function to generate samples from a Rayleigh($\sigma$) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X + X^{'}}{2}$   compared with $\frac{X_1 + X_2}{2}$ for independent $X_1, X_2$?


# Answer2

## Idea

å‡è®¾Uæ˜¯æ¥è‡ªå‡åŒ€åˆ†å¸ƒæ ·æœ¬é‡ä¸ºmçš„æ ·æœ¬ï¼Œåˆ™1-Uä¹Ÿæ˜¯æ¥è‡ªå‡åŒ€åˆ†å¸ƒæ ·æœ¬é‡ä¸ºmçš„æ ·æœ¬ï¼ŒæŒ‰ç…§é€†å˜æ¢æ³•$F_X(x)=u$æœä»å‡åŒ€åˆ†å¸ƒå…¶ä¸­Xæœä»ç‘åˆ©åˆ†å¸ƒï¼Œè€Œ$F_X(x)$æ˜¯ç‘åˆ©åˆ†å¸ƒåˆ†åˆ†å¸ƒå‡½æ•°ï¼Œæ ¹æ®è®¡ç®—ï¼Œç‘åˆ©åˆ†å¸ƒåˆ†å¸ƒå‡½æ•°ä¸º$$F_X(x)=1-e^{-\frac{x^2}{2\sigma^2}}$$äºæ˜¯$$X=\sqrt{-2\sigma^2ln(1-u)}$$å³å¯å¾—åˆ°æœä»ç‘åˆ©åˆ†å¸ƒçš„éšæœºå˜é‡ã€‚å…ˆè®¡ç®—æ–¹å·®åï¼ŒæŒ‰ç…§å¯¹å¶å˜é‡æ³•è®¡ç®—æ–¹å·®ï¼Œæœ€åè¿›è¡Œæ¯”è¾ƒ 

```{r }
knitr::opts_chunk$set(echo = TRUE)
# ç»™å‚æ•°èµ‹å€¼
m <- 1e4
sigma <- 1
# å¾—åˆ°æœä»å‡åŒ€åˆ†å¸ƒéšæœºå˜é‡uä»¥åŠ1-u
u <- runif(m)
u_anti <- 1-u
# é€†å˜æ¢æ³•å¾—åˆ°æœä»ç‘åˆ©åˆ†å¸ƒéšæœºå˜é‡xåŠå…¶å¯¹å¶éšæœºå˜é‡x_anti
x <- sqrt(-2*(sigma^2)*log(1-u))
x_anti <- sqrt(-2*(sigma^2)*log(1-u_anti))
# è®¡ç®—xçš„æ–¹å·®ï¼Œå¹¶æŒ‰ç…§å¯¹å¶å˜é‡æ³•è®¡ç®—æ–¹å·®ç„¶åå¯¹æ¯”
x_var = var(x)
x_anti_var = var((x+x_anti)/2)
reduce_per = 1 - x_anti_var/x_var
# æ‰“å°è®¡ç®—ç»“æœ
cat("åŸæ–¹å·®ä¸º", x_var)
cat("ç”¨å¯¹å¶å˜é‡æ³•å¾—åˆ°æ–¹å·®ä¸º", x_anti_var)
cat("æ–¹å·®ä¸ºé™ä½æ¯”ç‡ä¸º", reduce_per*100,"%")
```


# Problem3


Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are 'close' to

$$
g(x) = \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2}, \quad x > 1.
$$

Which of your two importance functions should produce the smaller variance in estimating

$$
\int_1^\infty \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2} \, dx
$$

by importance sampling? Explain.


# Answer3

## Idea

åŸå‡½æ•°g(x)æ˜¯ä¸€ä¸ªåœ¨$(1, \infty)$ä¸Šå•è°ƒé€’å‡çš„å‡½æ•°ï¼Œäºæ˜¯æƒ³åˆ°f_1(x)ç”¨æŒ‡æ•°å‡½æ•°é‡‡æ ·ï¼Œf_2(x)å¯ä»¥å–æ­£æ€åˆ†å¸ƒé‡‡æ ·ã€‚
 
## Code

```{r }
knitr::opts_chunk$set(echo = TRUE)
m <- 1e4
g <- function(x) {
  x^2 * exp(-x^2) * (x > 1)/sqrt(2 * pi)
}

x_1 <-  rexp(m, rate = 1)

fg_1 <- g(x_1)/exp(-x_1)
E_1 <- mean(fg_1)
se_1 <- sd(fg_1)
x_2 <-  rnorm(m)                
fg_2 <- g(x_1)/(exp(-x_2^2/2)/sqrt(2*pi))
E_2 <- mean(fg_2)
se_2 <- sd(fg_2)                                
cat("é€šè¿‡f_1é‡‡æ ·å¾—åˆ°çš„æ ‡å‡†å·®ä¸º:",se_1 , "\n")  
cat("é€šè¿‡f_2é‡‡æ ·å¾—åˆ°çš„æ ‡å‡†å·®ä¸º:",se_2, "\n")  
if (se_1 > se_2) {
  cat("æ­£æ€åˆ†å¸ƒä½œä¸ºé‡è¦å‡½æ•°æ›´æœ‰æ•ˆ\n")
} else {
  cat("æŒ‡æ•°åˆ†å¸ƒä½œä¸ºé‡è¦å‡½æ•°æ›´æœ‰æ•ˆ")
}

```



# Problem4

- For $n = 10^4, 2 \times 10^4, 4 \times 10^4, 6 \times 10^4, 8 \times 10^4$, apply the fast sorting algorithm to randomly permuted numbers of $1, \ldots, n$.
- Calculate computation time averaged over 100 simulations, denoted by $a_n$.
- Regress $a_n$ on $t_n := n \log(n)$, and graphically show the results (scatter plot and regression line).


# Answer4

## Idea
 
ä½¿ç”¨ QuickSort ç®—æ³•å¯¹ä¸åŒè§„æ¨¡ï¼ˆ1ä¸‡åˆ°8ä¸‡ï¼‰çš„éšæœºæ•°æ•°ç»„è¿›è¡Œæ’åºã€‚é¦–å…ˆï¼Œå®šä¹‰ä¸€ä¸ª n_listç”¨äºå­˜å‚¨ä¸åŒçš„æ•°ç»„å¤§å°ï¼Œå¹¶åˆå§‹åŒ– mean_time å‘é‡ä»¥å­˜å‚¨æ¯ä¸ªè§„æ¨¡çš„å¹³å‡è¿è¡Œæ—¶é—´ã€‚æ¥ç€ï¼Œå®šä¹‰é€’å½’çš„ QuickSort å‡½æ•°ï¼ŒåŸºäºåŸºå‡†å…ƒç´ å°†æ•°ç»„åˆ†ä¸ºä¸‰éƒ¨åˆ†ã€‚ä¸»å¾ªç¯ä¸­ï¼Œå¯¹æ¯ä¸ª n ç”Ÿæˆéšæœºæ’åˆ—ï¼Œæ‰§è¡Œ 100 æ¬¡æ’åºå¹¶è®°å½•æ—¶é—´ï¼Œè®¡ç®—å¹³å‡è¿è¡Œæ—¶é—´ã€‚æœ€åï¼Œé€šè¿‡æ•£ç‚¹å›¾å±•ç¤ºå¹³å‡è¿è¡Œæ—¶é—´ä¸ n çš„å…³ç³»ï¼Œå¹¶ç»˜åˆ¶ä¿®æ­£åçš„ n*log(n) æ›²çº¿è¿›è¡Œæ‹Ÿåˆåˆ†æã€‚ 
 
## Code

```{r }
knitr::opts_chunk$set(echo = TRUE)
# å®šä¹‰ n_list
n_list <- c(1e4, 2*1e4, 4*1e4, 6*1e4, 8*1e4)

# åˆå§‹åŒ– mean_time å‘é‡
mean_time <- numeric(length(n_list))  # é•¿åº¦ä¸ n_list ç›¸åŒ

# å®šä¹‰ QuickSort å‡½æ•°
quick_sort <- function(arr) {
  if (length(arr) <= 1) {
    return(arr)
  }
  
  # åŸºå‡†å…ƒç´ 
  pivot <- arr[1]
  
  # åˆ†åŒºæ“ä½œ
  left <- arr[arr < pivot]
  right <- arr[arr > pivot]
  equal <- arr[arr == pivot]
  
  # é€’å½’æ’åº
  return(c(quick_sort(left), equal, quick_sort(right)))
}

# ä¸»å¾ªç¯
for (i in 1:length(n_list)) {
  n <- n_list[i]  # å–å½“å‰ n çš„å€¼
  random_numbers <- sample(1:n)  # ç”Ÿæˆéšæœºæ’åˆ—
  execution_time <- numeric(100)  # åˆå§‹åŒ–å­˜å‚¨æ¯æ¬¡æ’åºæ—¶é—´çš„æ•°ç»„
  
  # è¿›è¡Œ 100 æ¬¡æ’åºå¹¶è®°å½•æ—¶é—´
  for (k in 1:100) {
    execution_time[k] <- system.time({
      sorted_numbers <- quick_sort(random_numbers)  # æ‰§è¡Œ QuickSort
    })[3]  # å– system.time çš„ç¬¬ä¸‰ä¸ªå…ƒç´ ï¼Œå³ elapsed æ—¶é—´
  }
  
  # è®¡ç®—å¹³å‡æ—¶é—´
  mean_time[i] <- mean(execution_time)
  
  # è¾“å‡ºæ¯æ¬¡çš„æ‰§è¡Œæ—¶é—´ä»¥åŠå¹³å‡æ—¶é—´
  cat("n =", n, "\n")
  cat("å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ï¼š", mean_time[i], "\n")
  
}
#ç”»å‡ºæ•£ç‚¹å›¾
plot(n_list, mean_time,
     main = "ç®—æ³•å¹³å‡è¿è¡Œæ—¶é—´ä¸nlog(n)çš„æ‹Ÿåˆå…³ç³»",
     xlab = "n",
     ylab = "å¹³å‡è¿è¡Œæ—¶é—´",)
#è®¡ç®—ä¿®æ­£ç³»æ•°
cons = mean_time[1]/(n_list[1]*log10(n_list[1]))
cat(cons)
#ç”»å‡ºn*log(n)ç»è¿‡ç³»æ•°ä¿®æ­£åçš„æ›²çº¿
n <- n_list
lines(n, n*log10(n)*cons) 
```

# ç¬¬ä¸‰æ¬¡ä½œä¸š

# Problem1
 
 Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness âˆšb1 under
 normality by a Monte Carlo experiment. Compute the standard error of the
 estimates from (2.14) using the normal approximation for the density (with
 exact variance formula). Compare the estimated quantiles with the quantiles
 of the large sample approximation âˆšb1 â‰ˆ N(0,6/n)
 
# Answer1

## Idea

é€šè¿‡å¤šæ¬¡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œå¾—åˆ°å¤šç»„æ­£æ€åˆ†å¸ƒæ ·æœ¬ï¼Œé€šè¿‡å¯¹æ¯ç»„æ ·æœ¬è¿›è¡Œè®¡ç®—ååº¦ï¼Œå¾—åˆ°ä¸€ç»„ååº¦å€¼ï¼Œåœ¨è¿™äº›ååº¦å€¼ä¸­æ‰¾åˆ°ç›¸åº”çš„åˆ†å¸ƒæ•°ä¸æ­£æ€åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒ

## Code

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(moments)  # ç”¨äºè®¡ç®—ååº¦

# è®¾ç½®å‚æ•°
n <- 100  # æ ·æœ¬é‡
m <- 1e5  # è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¬¡æ•°
alpha_list <- c(0.025, 0.05, 0.95, 0.975)  # è¦ä¼°è®¡çš„åˆ†ä½æ•°

# å‡½æ•°ç”¨äºç”Ÿæˆæ•°æ®å¹¶è®¡ç®—ååº¦ âˆšb1
skewness_sqrt <- function(n) {
  x <- rnorm(n)  # ä»æ­£æ€åˆ†å¸ƒä¸­ç”Ÿæˆæ•°æ®
  b1 <- skewness(x)  # è®¡ç®—ååº¦
  return(b1) # è¿”å› b1
}

# è¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
sim_skewness <- replicate(m, skewness_sqrt(n))

# ä¼°è®¡åˆ†ä½æ•°
quantiles_mc <- quantile(sim_skewness, probs = alpha_list)
print("è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿä¼°è®¡çš„åˆ†ä½æ•°ï¼š")
print(quantiles_mc)

# æ­£æ€è¿‘ä¼¼ N(0, 6/n)
quantiles_approx <- qnorm(alpha_list, mean = 0, sd = sqrt(6/n))
print("æ­£æ€è¿‘ä¼¼ä¼°è®¡çš„åˆ†ä½æ•°ï¼š")
print(quantiles_approx)

# è®¡ç®—æ ‡å‡†è¯¯å·®
std_error <- sd(sim_skewness) / sqrt(m)
cat("ä¼°è®¡çš„æ ‡å‡†è¯¯å·®:", std_error, "\n")


```

# Problem2
 
  Tests for association based on Pearson product moment correlation Ï,Spear
manâ€™s rank correlation coefficient Ïs, or Kendallâ€™s coefficient Ï„, are imple
mented in cor.test. Show (empirically) that the nonparametric tests based
 on Ïs or Ï„ are less powerful than the correlation test when the sampled dis
tribution is bivariate normal. Find an example of an alternative (a bivariate
 distribution (X,Y) such that X and Y are dependent) such that at least one
 of the nonparametric tests have better empirical power than the correlation
 test against this alternative
 
# Answer2

## Idea

éœ€è¦è¿›è¡Œä¸¤éƒ¨åˆ†å®éªŒåœ¨åŒå˜é‡æ­£æ€åˆ†å¸ƒä¸‹ï¼Œæ¯”è¾ƒåŸºäº Pearson ç›¸å…³ç³»æ•°ã€Spearman ç­‰çº§ç›¸å…³ç³»æ•°ã€Kendall ç³»æ•°çš„æ£€éªŒåŠŸæ•ˆã€‚
å¯»æ‰¾ä¸€ä¸ªéæ­£æ€åˆ†å¸ƒçš„ä¾‹å­ï¼Œå¹¶å±•ç¤ºåœ¨è¿™ç§åˆ†å¸ƒä¸‹ï¼Œéå‚æ•°æ£€éªŒçš„åŠŸæ•ˆä¼˜äº Pearson ç›¸å…³æ£€éªŒã€‚

## Code

```{r}
knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½å¿…è¦çš„åŒ…
install.packages("MASS", repos = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/")
library(MASS)

# å®šä¹‰æ ·æœ¬é‡å’Œæ¨¡æ‹Ÿæ¬¡æ•°
n <- 100  # æ ·æœ¬é‡
simulations <- 1000  # æ¨¡æ‹Ÿæ¬¡æ•°
alpha <- 0.05  # æ˜¾è‘—æ€§æ°´å¹³

# è®°å½•æ¯ç§æ£€éªŒçš„æ‹’ç»æ¬¡æ•°
pearson_rejects <- 0
spearman_rejects <- 0
kendall_rejects <- 0

# å®šä¹‰ç›¸å…³æ€§
true_rho <- 0.5  # å®é™…ç›¸å…³æ€§
sigma <- matrix(c(1, true_rho, true_rho, 1), 2, 2)

# æ¨¡æ‹ŸåŒå˜é‡æ­£æ€åˆ†å¸ƒä¸‹çš„æ£€éªŒåŠŸæ•ˆ
for (i in 1:simulations) {
  data <- mvrnorm(n, mu = c(0, 0), Sigma = sigma)
  
  # Pearson ç›¸å…³æ€§æ£€éªŒ
  pearson_test <- cor.test(data[,1], data[,2], method = "pearson")
  if (pearson_test$p.value < alpha) {
    pearson_rejects <- pearson_rejects + 1
  }
  
  # Spearman ç›¸å…³æ€§æ£€éªŒ
  spearman_test <- cor.test(data[,1], data[,2], method = "spearman")
  if (spearman_test$p.value < alpha) {
    spearman_rejects <- spearman_rejects + 1
  }
  
  # Kendall ç›¸å…³æ€§æ£€éªŒ
  kendall_test <- cor.test(data[,1], data[,2], method = "kendall")
  if (kendall_test$p.value < alpha) {
    kendall_rejects <- kendall_rejects + 1
  }
}

# è®¡ç®—ä¸‰ç§æ£€éªŒçš„åŠŸæ•ˆ
pearson_power <- pearson_rejects / simulations
spearman_power <- spearman_rejects / simulations
kendall_power <- kendall_rejects / simulations

cat("åŒå˜é‡æ­£æ€åˆ†å¸ƒä¸‹æ£€éªŒåŠŸæ•ˆï¼š\n")
cat("Pearson æ£€éªŒåŠŸæ•ˆ:", pearson_power, "\n")
cat("Spearman æ£€éªŒåŠŸæ•ˆ:", spearman_power, "\n")
cat("Kendall æ£€éªŒåŠŸæ•ˆ:", kendall_power, "\n")

# ----æ­¥éª¤ 2ï¼šéæ­£æ€åˆ†å¸ƒçš„ä¾‹å­----

# å®šä¹‰ä¸€ä¸ªéæ­£æ€åˆ†å¸ƒçš„ä¾‹å­ï¼ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰
for (i in 1:simulations) {
  X <- rlnorm(n, meanlog = 0, sdlog = 1)
  Y <- rlnorm(n, meanlog = 0, sdlog = 1)
  
  # Pearson ç›¸å…³æ€§æ£€éªŒ
  pearson_test <- cor.test(X, Y, method = "pearson")
  if (pearson_test$p.value < alpha) {
    pearson_rejects <- pearson_rejects + 1
  }
  
  # Spearman ç›¸å…³æ€§æ£€éªŒ
  spearman_test <- cor.test(X, Y, method = "spearman")
  if (spearman_test$p.value < alpha) {
    spearman_rejects <- spearman_rejects + 1
  }
  
  # Kendall ç›¸å…³æ€§æ£€éªŒ
  kendall_test <- cor.test(X, Y, method = "kendall")
  if (kendall_test$p.value < alpha) {
    kendall_rejects <- kendall_rejects + 1
  }
}

# è®¡ç®—ä¸‰ç§æ£€éªŒåœ¨éæ­£æ€åˆ†å¸ƒä¸‹çš„åŠŸæ•ˆ
pearson_power_non_normal <- pearson_rejects / simulations
spearman_power_non_normal <- spearman_rejects / simulations
kendall_power_non_normal <- kendall_rejects / simulations

cat("\néæ­£æ€åˆ†å¸ƒä¸‹æ£€éªŒåŠŸæ•ˆï¼š\n")
cat("Pearson æ£€éªŒåŠŸæ•ˆ:", pearson_power_non_normal, "\n")
cat("Spearman æ£€éªŒåŠŸæ•ˆ:", spearman_power_non_normal, "\n")
cat("Kendall æ£€éªŒåŠŸæ•ˆ:", kendall_power_non_normal, "\n")

```


# Problem3

- If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.
  - What is the corresponding hypothesis test problem?
  - What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?
  - Please provide the least necessary information for hypothesis testing.


# Answer3

## Idea

1. **å‡è®¾æ£€éªŒé—®é¢˜**ï¼š
   - åŸå‡è®¾ \( H_0 \)ï¼šä¸¤ç§æ–¹æ³•çš„ç»Ÿè®¡åŠŸæ•ˆç›¸åŒï¼Œå³ \( p_1 = p_2 \)ã€‚
   - å¤‡æ‹©å‡è®¾ \( H_a \)ï¼šä¸¤ç§æ–¹æ³•çš„ç»Ÿè®¡åŠŸæ•ˆä¸åŒï¼Œå³ \( p_1 \neq p_2 \)ã€‚

2. **é€‰æ‹©çš„æ£€éªŒæ–¹æ³•**ï¼š
   - ç”±äºæˆ‘ä»¬æ¯”è¾ƒçš„æ˜¯ä¸¤ä¸ªç‹¬ç«‹å®éªŒçš„æ¯”ä¾‹ï¼ˆ0.651 å’Œ 0.676ï¼‰ï¼Œåˆé€‚çš„æ£€éªŒæ–¹æ³•æ˜¯ **ä¸¤æ¯”ä¾‹Zæ£€éªŒ**ã€‚
   - **ä¸¤æ ·æœ¬tæ£€éªŒ** ä¸é€‚åˆï¼Œå› ä¸ºæˆ‘ä»¬å¤„ç†çš„æ˜¯æ¯”ä¾‹è€Œéå‡å€¼ã€‚
   - **é…å¯¹tæ£€éªŒ** ä¸é€‚ç”¨ï¼Œå› ä¸ºä¸¤ç§æ–¹æ³•æ˜¯ç‹¬ç«‹çš„ã€‚
   - **McNemaræ£€éªŒ** é€‚ç”¨äºæˆå¯¹çš„åä¹‰æ•°æ®ï¼Œä¸é€‚ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¯”ä¾‹ã€‚

3. **æ‰€éœ€çš„å¿…è¦ä¿¡æ¯**ï¼š
   - ä¸¤ä¸ªè§‚æµ‹æ¯”ä¾‹ï¼š0.651 å’Œ 0.676ã€‚
   - æ¯ç§æ–¹æ³•çš„å®éªŒæ€»æ¬¡æ•°ï¼ˆå‡è®¾ä¸º10,000æ¬¡ï¼‰ã€‚
   - æ˜¾è‘—æ€§æ°´å¹³ï¼ˆ\( \alpha \)ï¼‰= 0.05ã€‚

# ç¬¬å››æ¬¡ä½œä¸š

# Problem1
 
 Of N =1000 hypotheses, 950 are null and 50 are alternative.
 The p-value under any null hypothesis is uniformly distributed
 (use runif), and the p-value under any alternative hypothesis
 follows the beta distribution with parameter 0.1 and 1 (use
 rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted
 p-values. Calculate FWER, FDR, and TPR under nominal level
 Î± =0.1 for each of the two adjustment methods based on
 m=10000 simulation replicates. You should output the 6
 numbers (3 ) to a 3Ã—2 table (column names: Bonferroni
 correction, B-H correction; row names: FWER, FDR, TPR).
 Comment the results
 
# Answer1

## Idea

ç”Ÿæˆ950ä¸ªç©ºå‡è®¾çš„på€¼ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰å’Œ50ä¸ªå¤‡æ‹©å‡è®¾çš„på€¼ï¼ˆBetaåˆ†å¸ƒï¼‰ã€‚æ¥ç€ï¼Œä½¿ç”¨Bonferroniå’ŒB-Hä¸¤ç§æ ¡æ­£æ–¹æ³•è°ƒæ•´på€¼ã€‚ç„¶åè®¡ç®—FWERï¼ˆå®¶æ—é”™è¯¯ç‡ï¼‰ã€FDRï¼ˆå‡å‘ç°ç‡ï¼‰å’ŒTPRï¼ˆçœŸé˜³æ€§ç‡ï¼‰ã€‚é€šè¿‡è¿›è¡Œ10,000æ¬¡æ¨¡æ‹Ÿï¼Œæ¯æ¬¡è®¡ç®—Bonferroniå’ŒB-Hæ ¡æ­£åçš„FWERã€FDRå’ŒTPRï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä¸ª3x2çš„è¡¨æ ¼

## Code

```{r}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)  # è®¾ç½®éšæœºç§å­

# å®šä¹‰å‚æ•°
N <- 1000      # æ€»å‡è®¾æ•°é‡
m <- 10000     # æ¨¡æ‹Ÿæ¬¡æ•°
alpha <- 0.1   # æ˜¾è‘—æ€§æ°´å¹³

# åˆå§‹åŒ–å­˜å‚¨ FWER, FDR å’Œ TPR çš„çŸ©é˜µ
results <- matrix(0, nrow=3, ncol=2)
colnames(results) <- c("Bonferroni", "B-H")
rownames(results) <- c("FWER", "FDR", "TPR")

# æ¨¡æ‹Ÿè¿‡ç¨‹
for (sim in 1:m) {
  # ç”Ÿæˆ 950 ä¸ªç©ºå‡è®¾çš„ p å€¼ (å‡åŒ€åˆ†å¸ƒ) å’Œ 50 ä¸ªå¤‡æ‹©å‡è®¾çš„ p å€¼ (betaåˆ†å¸ƒ)
  p_null <- runif(950)
  p_alt <- rbeta(50, 0.1, 1)
  
  # åˆå¹¶ p å€¼
  p_values <- c(p_null, p_alt)
  
  # Bonferroni æ ¡æ­£ï¼šp å€¼ä¹˜ä»¥å‡è®¾æ•°é‡
  p_bonferroni <- p.adjust(p_values, method = "bonferroni")
  
  # Benjamini-Hochberg (B-H) æ ¡æ­£
  p_bh <- p.adjust(p_values, method = "BH")
  
  # Bonferroni æ ¡æ­£ä¸‹çš„æ£€éªŒç»“æœ
  rejected_bonferroni <- p_bonferroni < alpha
  rejected_bh <- p_bh < alpha
  
  # è®¡ç®—æŒ‡æ ‡
  # FWER: è‡³å°‘ä¸€ä¸ªå‡è®¾é”™è¯¯æ‹’ç»çš„æ¦‚ç‡ï¼ˆå…¨ç©ºå‡è®¾ä¸­æ‹’ç»çš„æ•°ç›® > 0ï¼‰
  results["FWER", "Bonferroni"] <- results["FWER", "Bonferroni"] + any(rejected_bonferroni[1:950])
  results["FWER", "B-H"] <- results["FWER", "B-H"] + any(rejected_bh[1:950])
  
  # FDR: é”™è¯¯æ‹’ç»å æ‰€æœ‰æ‹’ç»çš„æ¯”ä¾‹
  FDR_bonferroni <- sum(rejected_bonferroni[1:950]) / max(1, sum(rejected_bonferroni))
  FDR_bh <- sum(rejected_bh[1:950]) / max(1, sum(rejected_bh))
  
  results["FDR", "Bonferroni"] <- results["FDR", "Bonferroni"] + FDR_bonferroni
  results["FDR", "B-H"] <- results["FDR", "B-H"] + FDR_bh
  
  # TPR: çœŸé˜³æ€§ï¼ˆå¤‡æ‹©å‡è®¾ï¼‰ä¸­è¢«æ­£ç¡®æ£€æµ‹åˆ°çš„æ¯”ä¾‹
  TPR_bonferroni <- sum(rejected_bonferroni[951:1000]) / 50
  TPR_bh <- sum(rejected_bh[951:1000]) / 50
  
  results["TPR", "Bonferroni"] <- results["TPR", "Bonferroni"] + TPR_bonferroni
  results["TPR", "B-H"] <- results["TPR", "B-H"] + TPR_bh
}

# å¹³å‡åŒ–ç»“æœ
results <- results / m

# è¾“å‡ºç»“æœè¡¨
print(results)


```


# Problem2
 
Refer to the air-conditioning data set aircondit provided in the boot pack
age. The 12 observations are the times in hours between failures of air
conditioning equipment [63, Example 1.1]:
 3, 5,7,18,43,85,91,98,100,130,230,487.
 Assume that the times between failures follow an exponential model Exp(Î»).
 Obtain the MLE of the hazard rate Î» and use bootstrap to estimate the bias
 and standard error of the estimate.
 
# Answer2

## Idea

åœ¨æŒ‡æ•°åˆ†å¸ƒä¸‹ï¼Œå‚æ•° Î» çš„MLEä¸ºè§‚æµ‹å€¼çš„å€’æ•°å‡å€¼ã€‚ä½¿ç”¨è‡ªåŠ©æ³• (Bootstrap) ä»æ•°æ®ä¸­è¿›è¡Œé‡å¤æŠ½æ ·ï¼Œä¼°è®¡æ¯æ¬¡æ ·æœ¬ä¸­çš„ Î»ã€‚è®¡ç®—åå·®ï¼šåå·® = å¹³å‡ä¼°è®¡å€¼ - MLEã€‚è®¡ç®—æ ‡å‡†è¯¯ï¼šæ ·æœ¬ä¼°è®¡å€¼çš„æ ‡å‡†å·®

## Code

```{r}
knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½å¿…è¦çš„åŒ…
library(boot)

# ç»™å®šçš„ç©ºè°ƒæ•…éšœæ—¶é—´æ•°æ®
data <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)

# æ±‚è§£ MLE for Î»
lambda_mle <- 1 / mean(data)
cat("MLE for Î»:", lambda_mle, "\n")

# è‡ªåŠ©æ³•ä¼°è®¡åå·®å’Œæ ‡å‡†è¯¯
bootstrap_lambda <- function(data, indices) {
  # è·å–è‡ªåŠ©æ³•æ ·æœ¬
  sample_data <- data[indices]
  # è®¡ç®—è‡ªåŠ©æ³•æ ·æœ¬ä¸­çš„Î»
  return(1 / mean(sample_data))
}

# ä½¿ç”¨bootå‡½æ•°è¿›è¡Œ1000æ¬¡è‡ªåŠ©æ³•é‡å¤
set.seed(123)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤
boot_results <- boot(data, bootstrap_lambda, R = 1000)

# è®¡ç®—åå·®å’Œæ ‡å‡†è¯¯
bias <- mean(boot_results$t) - lambda_mle
std_error <- sd(boot_results$t)

cat("Bootstrap bias estimate:", bias, "\n")
cat("Bootstrap standard error estimate:", std_error, "\n")

# æŸ¥çœ‹è‡ªåŠ©æ³•ç»“æœ
print(boot_results)

```


# Problem3
 
 Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
 mean time between failures 1/Î» by the standard normal, basic, percentile,
 and BCa methods. Compare the intervals and explain why they may differ.
 
# Answer3

## Idea

è®¡ç®—æ•…éšœé—´éš”æ—¶é—´å‡å€¼ï¼ˆå³1/Î»ï¼‰çš„95%è‡ªåŠ©æ³•ç½®ä¿¡åŒºé—´ï¼Œåˆ†åˆ«ä½¿ç”¨æ ‡å‡†æ­£æ€ã€åŸºæœ¬ã€ç™¾åˆ†ä½ã€ä»¥åŠBCaæ–¹æ³•è¿›è¡Œä¼°è®¡ï¼Œå¹¶æ¯”è¾ƒè¿™äº›ç½®ä¿¡åŒºé—´ã€‚

## Code

```{r}
knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½å¿…è¦çš„åŒ…
library(boot)

# ç»™å®šçš„ç©ºè°ƒæ•…éšœæ—¶é—´æ•°æ®
data <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)

# è‡ªåŠ©æ³•ä¼°è®¡ 1/Î»ï¼ˆæ•…éšœé—´éš”å‡å€¼ï¼‰
bootstrap_mean_time <- function(data, indices) {
  sample_data <- data[indices]
  lambda_sample <- 1 / mean(sample_data)
  return(1 / lambda_sample)  # è¿”å›æ•…éšœé—´éš”å‡å€¼ï¼Œå³ 1/Î»
}

# ä½¿ç”¨bootå‡½æ•°è¿›è¡Œ1000æ¬¡è‡ªåŠ©æ³•é‡å¤
set.seed(123)
boot_results <- boot(data, bootstrap_mean_time, R = 1000)

# è®¡ç®—95%çš„ç½®ä¿¡åŒºé—´ï¼šæ ‡å‡†æ­£æ€ã€åŸºæœ¬æ³•ã€ç™¾åˆ†ä½æ³•ã€BCaæ³•
ci_normal <- boot.ci(boot_results, type = "norm")
ci_basic <- boot.ci(boot_results, type = "basic")
ci_percentile <- boot.ci(boot_results, type = "perc")
ci_bca <- boot.ci(boot_results, type = "bca")

# è¾“å‡ºæ‰€æœ‰çš„ç½®ä¿¡åŒºé—´
cat("Standard Normal CI:", ci_normal$normal[2:3], "\n")
cat("Basic CI:", ci_basic$basic[4:5], "\n")
cat("Percentile CI:", ci_percentile$percent[4:5], "\n")
cat("BCa CI:", ci_bca$bca[4:5], "\n")


```

## Conclusion

æ ‡å‡†æ­£æ€æ³•å‡è®¾å¯¹ç§°çš„æ­£æ€åˆ†å¸ƒï¼Œè€ŒåŸºæœ¬æ³•ã€ç™¾åˆ†ä½æ³•å’ŒBCaæ³•é€šå¸¸ä¼šç»™å‡ºä¸å¯¹ç§°çš„åŒºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®åæ€æ—¶ã€‚BCaæ–¹æ³•é€šå¸¸èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ åæ€æ•°æ®çš„çœŸå®æƒ…å†µï¼Œå› æ­¤åœ¨åæ€æ•°æ®ä¸­å¯èƒ½æ¯”å…¶ä»–æ–¹æ³•æ›´å¯é ã€‚


# ç¬¬äº”æ¬¡ä½œä¸š

# Problem1
 
 Efron and Tibshirani discuss the scor (bootstrap) test score data on 88 students who took examinations in five subjects .The first two tests (mechanics, vectors) were closed book and the last three
tests (algebra, analysis, statistics) were open book. Each row of the data
frame is a set of scores (xi1,...,xi5) for the i. The five-dimensional scores data have a 5 Ã— 5 covariance matrix Î£,
with positive eigenvalues Î»1 > Â·Â·Â· > Î»5. In principal components analysis,
measures the proportion of variance explained by the first principal component. Let Î»Ë†1 > Â·Â·Â· > Î»Ë†5 be the eigenvalues of Î£, where Ë† Î£ is the MLE of Î£. Ë†
Compute the sample estimateË†Î¸ = Î»Ë†1/Î£ Î»Ë†.Obtain the jackknife estimates of bias and standard
error of Ë†Î¸


# Answer1

## Idea

é¦–å…ˆï¼Œæ ¹æ®é¢˜ç›®æä¾›çš„æ•°æ®ï¼Œè®¡ç®— 5x5 æ ·æœ¬åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ã€‚è®¡ç®—å‚æ•° 
ğœƒçš„ä¼°è®¡å€¼ ğœƒË†ï¼Œå®ƒè¡¨ç¤ºç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ã€‚ä¾æ¬¡ç§»é™¤æ¯ä¸ªæ•°æ®ç‚¹ï¼Œè®¡ç®—ç§»é™¤è¯¥ç‚¹åçš„åæ–¹å·®çŸ©é˜µç‰¹å¾å€¼ï¼Œå¾—åˆ°æ–°çš„ ğœƒË†ã€‚æœ€åæ ¹æ®Jackknife å‡å€¼ä¸æ–¹å·®å…¬å¼è®¡ç®— Jackknife åå·®

## Code

```{r}

library(bootstrap) 
# åŠ è½½ `scor` æ•°æ®é›†
data("scor", package = "bootstrap")

# è®¡ç®—åæ–¹å·®çŸ©é˜µå’Œå…¶ç‰¹å¾å€¼
cov_matrix <- cov(scor)
eigenvalues <- eigen(cov_matrix)$values

# æ ¹æ® 7.7 çš„å…¬å¼è®¡ç®— \hat{Î¸}
theta_hat <- eigenvalues[1] / sum(eigenvalues)
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºè®¡ç®— Î¸
theta_fn <- function(data, indices) {
  cov_matrix <- cov(data[indices, ])
  eigenvalues <- eigen(cov_matrix)$values
  eigenvalues[1] / sum(eigenvalues)
}

# è·å–æ•°æ®é›†å¤§å°
n <- nrow(scor)
theta_jackknife <- numeric(n)

# è®¡ç®—æ¯ä¸ª Jackknife æ ·æœ¬çš„ Î¸ å€¼
for (i in 1:n) {
  theta_jackknife[i] <- theta_fn(scor, (1:n)[-i])
}

# Jackknife åå·®ä¼°è®¡
theta_bar <- mean(theta_jackknife)
jackknife_bias <- (n - 1) * (theta_bar - theta_hat)
cat("åå·®å¤§å°ä¸º:",jackknife_bias)

# Jackknife æ ‡å‡†è¯¯å·®ä¼°è®¡
jackknife_se <- sqrt((n - 1) * mean((theta_jackknife - theta_bar)^2))
cat("æ–¹å·®å¤§å°ä¸º:",jackknife_se)

```


# Problem2
 
In Example 7.18, leave-one-out (n-fold) cross validation was used to select
the best fitting model. Repeat the analysis replacing the Log-Log model
with a cubic polynomial model. Which of the four models is selected by the
cross validation procedure? Which model is selected according to maximum
adjusted R2

 
# Answer2

## Idea

åŠ è½½ironslagæ•°æ®é›†ï¼Œè¿™å…¶ä¸­åŒ…å«53ä¸ªè§‚æµ‹å€¼ï¼Œæ¯ä¸ªè§‚æµ‹å€¼æœ‰ä¸¤ä¸ªå˜é‡ï¼Œå³åŒ–å­¦æ–¹æ³•æµ‹å¾—çš„é“å«é‡ï¼‰å’Œç£æ€§æ–¹æ³•æµ‹å¾—çš„é“å«é‡ã€‚æ ¹æ®é¢˜æ„ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å››ä¸ªæ¨¡å‹è¿›è¡Œæ‹Ÿåˆã€‚ä¸ºäº†æ‰¾åˆ°æœ€ä¼˜æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼ˆLOOCVï¼‰æ¥è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹è¯¯å·®ã€‚æ­¤å¤–ï¼Œè®¡ç®—æ¯ä¸ªæ¨¡å‹çš„è°ƒæ•´åçš„ $R^2$ å€¼ã€‚æ ¹æ®äº¤å‰éªŒè¯å¾—åˆ°çš„å¹³å‡é¢„æµ‹è¯¯å·®é€‰æ‹©æœ€ä¼˜æ¨¡å‹ã€‚æœ€åæ ¹æ®æœ€å¤§è°ƒæ•´åçš„ $R^2$ é€‰æ‹©æœ€ä¼˜æ¨¡å‹ã€‚

## Code

```{r}

library(DAAG)
library(boot)
data(ironslag)
par(mfrow=c(2,2))
n <- nrow(ironslag)
errors <- numeric(n)

# 1. Linear model
L1 <- lm(magnetic ~ chemical, data=ironslag)
plot(ironslag$chemical, ironslag$magnetic, main="Linear Model", pch=16)
a <- seq(10, 40, 0.1)
lines(a, predict(L1, newdata=data.frame(chemical=a)), col="blue", lwd=2)

# 2. Quadratic model
L2 <- lm(magnetic ~ chemical + I(chemical^2), data=ironslag)
plot(ironslag$chemical, ironslag$magnetic, main="Quadratic Model", pch=16)
lines(a, predict(L2, newdata=data.frame(chemical=a)), col="blue", lwd=2)

# 3. Exponential model
L3 <- lm(log(magnetic) ~ chemical, data=ironslag)
plot(ironslag$chemical, ironslag$magnetic, main="Exponential Model", pch=16)
lines(a, exp(predict(L3, newdata=data.frame(chemical=a))), col="blue", lwd=2)

# 4. Cubic model (replacing Log-Log model)
L4 <- lm(magnetic ~ chemical + I(chemical^2) + I(chemical^3), data=ironslag)
plot(ironslag$chemical, ironslag$magnetic, main="Cubic Model", pch=16)
lines(a, predict(L4, newdata=data.frame(chemical=a)), col="blue", lwd=2)

cv_error <- function(model_formula, data) {
  glm_fit <- glm(model_formula, data=data)
  cv <- cv.glm(data, glm_fit, K=nrow(data))
  return(cv$delta[1])  # LOOCV Error
}

for (i in 1:n) {
  # ç•™ä¸€æ³•åˆ†å‰²æ•°æ®
  train_data <- ironslag[-i, ]
  test_data <- ironslag[i, ]
  
  # è®­ç»ƒ Exponential æ¨¡å‹
  model <- lm(log(magnetic) ~ chemical, data = train_data)
  
  # åœ¨å¯¹æ•°å°ºåº¦ä¸Šç”Ÿæˆé¢„æµ‹
  log_pred <- predict(model, newdata = test_data)
  
  # å°†é¢„æµ‹å€¼ä»å¯¹æ•°å°ºåº¦è½¬æ¢å›åŸå§‹å°ºåº¦
  pred <- exp(log_pred)
  
  # è®¡ç®—è¯¯å·®å¹¶å­˜å‚¨
  errors[i] <- (test_data$magnetic - pred)^2
}
  loocv_error_Exponential <- mean(errors)  

# è®¡ç®— LOOCV è¯¯å·®
loocv_errors <- c(
  Linear = cv_error(magnetic ~ chemical, ironslag),
  Quadratic = cv_error(magnetic ~ chemical + I(chemical^2), ironslag),
  Exponential = loocv_error_Exponential,
  Cubic = cv_error(magnetic ~ chemical + I(chemical^2) + I(chemical^3), ironslag)
)

loocv_errors

adjusted_r_squared <- c(
  Linear = summary(L1)$adj.r.squared,
  Quadratic = summary(L2)$adj.r.squared,
  Exponential = summary(lm(magnetic ~ fitted(L3), ironslag))$adj.r.squared,
  Cubic = summary(L4)$adj.r.squared
)

adjusted_r_squared

# æœ€ä½³LOOCVæ¨¡å‹
best_model_loocv <- names(which.min(loocv_errors))
cat("æœ€ä½³LOOCVæ¨¡å‹:", best_model_loocv, "\n")

# è°ƒæ•´åR^2-basedæœ€ä½³æ¨¡å‹
best_model_adj_r2 <- names(which.max(adjusted_r_squared))
cat("è°ƒæ•´åR^2-basedæœ€ä½³æ¨¡å‹:", best_model_adj_r2, "\n")



```

# Problem3
 
 Implement the two-sample CramÂ´er-von Mises test for equal distributions as a
permutation test. Apply the test to the data in Examples 8.1 and 8.2.
 
# Answer3

## Idea

å®šä¹‰å‡½æ•° cramer_von_mises æ¥è®¡ç®—è§‚å¯Ÿåˆ°çš„ç»Ÿè®¡é‡ã€‚è¿›è¡Œ999æ¬¡ç½®æ¢ï¼Œé€šè¿‡éšæœºæ‰“ä¹±æ ·æœ¬æ¥ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Œå¹¶è®¡ç®—å¯¹åº”çš„ç»Ÿè®¡é‡ã€‚é€šè¿‡æ¯”è¾ƒç½®æ¢ç»Ÿè®¡é‡å’Œè§‚å¯Ÿåˆ°çš„ç»Ÿè®¡é‡æ¥è®¡ç®—på€¼ã€‚

## Code

```{r}

# åŠ è½½å¿…è¦çš„åº“
set.seed(123)  # ä¸ºäº†å¯é‡å¤æ€§

# ç¤ºä¾‹æ•°æ®
x <- c(158, 171, 193, 199, 230, 243, 248, 248, 250, 267, 271, 316, 327, 329)
y <- c(141, 148, 169, 181, 203, 213, 229, 244, 257, 260, 271, 309)

# è®¡ç®— CramÃ©r-von Mises ç»Ÿè®¡é‡
cramer_von_mises <- function(x, y) {
  n_x <- length(x)
  n_y <- length(y)
  combined <- c(x, y)
  n <- n_x + n_y
  ranks <- rank(combined)
  
  # è®¡ç®— F_X å’Œ F_Y
  rank_x <- ranks[1:n_x]
  rank_y <- ranks[(n_x + 1):n]
  
  # è®¡ç®— W^2
  W2 <- (1/(n_x * n_y)) * (sum((rank_x / n)^2) + sum((rank_y / n)^2) - (n / (n + 1))^2)
  return(W2)
}

# è§‚å¯Ÿåˆ°çš„ CramÃ©r-von Mises ç»Ÿè®¡é‡
D0 <- cramer_von_mises(x, y)

# è¿›è¡Œç½®æ¢æ£€éªŒ
R <- 10000  # ç½®æ¢æ¬¡æ•°
z <- c(x, y)  # åˆå¹¶æ ·æœ¬
D <- numeric(R)  # å­˜å‚¨ç½®æ¢ç»Ÿè®¡é‡

for (i in 1:R) {
  # éšæœºæ‰“ä¹±æ•°æ®
  shuffled <- sample(z)
  
  # ç”Ÿæˆæ–°çš„æ ·æœ¬
  new_x <- shuffled[1:length(x)]
  new_y <- shuffled[(length(x) + 1):length(z)]
  
  # è®¡ç®—æ–°çš„ CramÃ©r-von Mises ç»Ÿè®¡é‡
  D[i] <- cramer_von_mises(new_x, new_y)
}

# è®¡ç®—på€¼
p_value <- mean(c(D0, D) >= D0)

# æ˜¾ç¤ºç»“æœ
cat("è§‚å¯Ÿåˆ°çš„ CramÃ©r-von Mises ç»Ÿè®¡é‡:", D0, "\n")
cat("ç½®æ¢æ£€éªŒçš„ p å€¼:", p_value, "\n")


```

# Problem4
 
Implement the bivariate Spearman rank correlation test for independence
[255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the
achieved significance level of the permutation test with the p-value reported
by cor.test on the same samples. 
 
# Answer4

## Idea

åˆ›å»ºä¸¤ä¸ªå˜é‡ä»¥æµ‹è¯•ç›¸å…³æ€§ï¼Œç„¶åä½¿ç”¨Rä¸­çš„corå‡½æ•°è®¡ç®—åŸå§‹æ•°æ®çš„æ–¯çš®å°”æ›¼ç›¸å…³æ€§ã€‚éšæœºç½®æ¢å…¶ä¸­ä¸€ä¸ªå˜é‡å¤šæ¬¡ã€‚å¯¹äºæ¯æ¬¡ç½®æ¢ï¼Œè®¡ç®—æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°ã€‚ç„¶åæ„å»ºæ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°çš„ç½®æ¢åˆ†å¸ƒï¼Œç¡®å®šç½®æ¢çš„æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°ä¸­æœ‰å¤šå°‘æ¯”ä¾‹ä¸è§‚å¯Ÿåˆ°çš„å€¼åŒæ ·æç«¯ï¼Œæœ€åä¸cor.testçš„på€¼è¿›è¡Œæ¯”è¾ƒã€‚

## Code

```{r}

# åŠ è½½å¿…è¦çš„åº“
set.seed(123)  # ä¸ºäº†å¯é‡å¤æ€§


n <- 200  # å¢åŠ æ ·æœ¬é‡
x <- rnorm(n)  # ç”Ÿæˆæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ•°æ®
y <- 2 * sin(2 * pi * x) + rnorm(n, sd = 0.5)  # å¼•å…¥éçº¿æ€§å…³ç³»å’Œå™ªå£°

# è®¡ç®—è§‚å¯Ÿåˆ°çš„æ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°
observed_cor <- cor(x, y, method = "spearman")

# è¿›è¡Œç½®æ¢æ£€éªŒ
n_permutations <- 10000  # å¢åŠ ç½®æ¢æ¬¡æ•°
permuted_correlations <- numeric(n_permutations)

for (i in 1:n_permutations) {
  # ç½®æ¢y
  y_permuted <- sample(y)
  # è®¡ç®—ç½®æ¢æ•°æ®çš„æ–¯çš®å°”æ›¼ç›¸å…³æ€§
  permuted_correlations[i] <- cor(x, y_permuted, method = "spearman")
}

# è®¡ç®—ç½®æ¢æ£€éªŒçš„på€¼
p_value_perm <- mean(abs(permuted_correlations) >= abs(observed_cor))

# ä¸cor.testçš„på€¼è¿›è¡Œæ¯”è¾ƒ
cor_test_result <- cor.test(x, y, method = "spearman")
p_value_cor_test <- cor_test_result$p.value

# æ˜¾ç¤ºç»“æœ
cat("è§‚å¯Ÿåˆ°çš„æ–¯çš®å°”æ›¼ç›¸å…³æ€§ç³»æ•°:", observed_cor, "\n")
cat("ç½®æ¢æ£€éªŒpå€¼:", p_value_perm, "\n")
cat("cor.testçš„på€¼:", p_value_cor_test, "\n")


```



# ç¬¬å…­æ¬¡ä½œä¸š

# Problem1
 
Use the Metropolis-Hastings sampler to generate random variables from a
standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard 
Cauchy distribution (see qcauchy or qt with df=1).The standard Cauchy has the Cauchy(Î¸ = 1, Î· = 0) density. (Note that the
standard Cauchy density is equal to the Student t density with one degree of
freedom.)

# Answer1

## Idea

ä½¿ç”¨ Metropolis-Hastings é‡‡æ ·å™¨ç”Ÿæˆæ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„éšæœºå˜é‡ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ç”Ÿæˆä¸€ä¸ªå€™é€‰ç‚¹ proposalï¼Œè¯¥ç‚¹ä»ä»¥å½“å‰çŠ¶æ€ä¸ºå‡å€¼ã€æ ‡å‡†å·®ä¸º 1 çš„æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·ã€‚
è®¡ç®—æ¥å—ç‡ alphaï¼Œå³ç›®æ ‡åˆ†å¸ƒåœ¨å€™é€‰ç‚¹çš„å€¼å’Œå½“å‰çŠ¶æ€å€¼çš„æ¯”å€¼ã€‚æ¥å—ç‡ä¸å¤§äº 1ã€‚
ç”Ÿæˆä¸€ä¸ªå‡åŒ€éšæœºæ•°ï¼Œå¦‚æœå°äº alpha åˆ™æ¥å—å€™é€‰ç‚¹ï¼Œå¦åˆ™ä¿æŒå½“å‰çŠ¶æ€ä¸å˜ã€‚

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# è®¾ç½®å‚æ•°
n <- 10000  # ç”Ÿæˆæ ·æœ¬çš„æ€»æ•°
burn_in <- 1000  # èˆå¼ƒçš„å‰ 1000 ä¸ªå€¼
initial_value <- 0  # é“¾çš„åˆå§‹å€¼

# ç›®æ ‡åˆ†å¸ƒï¼šæ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°
target_density <- function(x) {
  return(1 / (pi * (1 + x^2)))
}

# Metropolis-Hastings ç®—æ³•å®ç°
set.seed(123)  # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
chain <- numeric(n)  # ç”¨äºå­˜å‚¨ç”Ÿæˆçš„æ ·æœ¬
chain[1] <- initial_value  # åˆå§‹åŒ–é“¾çš„ç¬¬ä¸€ä¸ªå€¼

for (i in 2:n) {
  # ç”Ÿæˆå€™é€‰ç‚¹ï¼Œé‡‡ç”¨å¯¹ç§°çš„æ­£æ€åˆ†å¸ƒä½œä¸ºæè®®åˆ†å¸ƒ
  proposal <- rnorm(1, mean = chain[i - 1], sd = 1)
  
  # è®¡ç®—æ¥å—ç‡ alpha
  alpha <- min(1, target_density(proposal) / target_density(chain[i - 1]))
  
  # æ ¹æ®æ¥å—ç‡å†³å®šæ˜¯å¦æ¥å—å€™é€‰ç‚¹
  if (runif(1) < alpha) {
    chain[i] <- proposal
  } else {
    chain[i] <- chain[i - 1]
  }
}

# èˆå¼ƒå‰ 1000 ä¸ªæ ·æœ¬ï¼Œå¾—åˆ°å®é™…çš„æ ·æœ¬
samples <- chain[(burn_in + 1):n]
# ç”Ÿæˆæ ·æœ¬çš„åˆ†ä½æ•°
sample_deciles <- quantile(samples, probs = seq(0.1, 0.9, by = 0.1))

# æ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„ç†è®ºåˆ†ä½æ•°
cauchy_deciles <- qcauchy(seq(0.1, 0.9, by = 0.1))

# æ¯”è¾ƒç»“æœ
comparison <- data.frame(
  Decile = seq(0.1, 0.9, by = 0.1),
  Sample_Deciles = sample_deciles,
  Cauchy_Deciles = cauchy_deciles
)
print(comparison)


```


# Problem2
 
 This example appears in [40]. Consider the bivariate density 
 $$
f(x, y) \propto \binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1}, \quad x = 0, 1, \dots, n, \quad 0 \leq y \leq 1.
$$

It can be shown (see e.g. [23]) that for fixed a, b, n, the conditional distributions are Binomial(n, y) and Beta(x + a, n âˆ’ x + b). Use the Gibbs sampler to
generate a chain with target joint density f(x, y).

# Answer2

## Idea

1. ç»™å®š$ y $ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¡ä»¶åˆ†å¸ƒ $ x | y \sim \text{Binomial}(n, y) $ ä¸­é‡‡æ ·ã€‚
2. ç»™å®š$x $ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¡ä»¶åˆ†å¸ƒ $ y | x \sim \text{Beta}(x + a, n - x + b) $ ä¸­é‡‡æ ·ã€‚

é€šè¿‡é‡å¤è¿™ä¸¤ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆç¬¦åˆç›®æ ‡è”åˆå¯†åº¦ $ f(x, y) $ çš„æ ·æœ¬ã€‚

## Code

```{r}

# è®¾ç½®å‚æ•°
n <- 10
a <- 2
b <- 2
num_samples <- 10000  # ç”Ÿæˆæ ·æœ¬æ•°é‡

# åˆå§‹åŒ– x å’Œ y
x <- 0
y <- 0.5
samples <- matrix(NA, nrow = num_samples, ncol = 2)  # ç”¨äºå­˜å‚¨ç”Ÿæˆçš„ (x, y) æ ·æœ¬

# Gibbs é‡‡æ ·
set.seed(123)
for (i in 1:num_samples) {
  # ä» x | y çš„æ¡ä»¶åˆ†å¸ƒä¸­é‡‡æ · (Binomial distribution)
  x <- rbinom(1, n, y)
  
  # ä» y | x çš„æ¡ä»¶åˆ†å¸ƒä¸­é‡‡æ · (Beta distribution)
  y <- rbeta(1, x + a, n - x + b)
  
  # ä¿å­˜æ ·æœ¬
  samples[i, ] <- c(x, y)
}

# å°†é‡‡æ ·ç»“æœå­˜å…¥æ•°æ®æ¡†ä¸­
samples_df <- as.data.frame(samples)
colnames(samples_df) <- c("x", "y")

# ç»˜åˆ¶é‡‡æ ·ç»“æœ
library(ggplot2)
ggplot(samples_df, aes(x = x, y = y)) +
  geom_point(alpha = 0.3) +
  labs(title = "Gibbs Sampling Results", x = "x", y = "y") +
  theme_minimal()

```


# Problem3
 
For each of the above exercise, use the Gelman-Rubin method
to monitor convergence of the chain, and run the chain until it
converges approximately to the target distribution according to
RË† < 1.2

# Answer3

## 1)Idea

ä½¿ç”¨ Metropolis-Hastings é‡‡æ ·å™¨ç”Ÿæˆæ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œå¹¶é€šè¿‡ Gelman-Rubin è¯Šæ–­æ¥ç›‘æµ‹æ”¶æ•›æƒ…å†µã€‚æ¯æ¬¡è¿­ä»£è®¡ç®— Gelman-Rubin è¯Šæ–­ç›´åˆ°æ»¡è¶³ 
ğ‘…^<1.2ï¼Œè¡¨ç¤ºé“¾æ¡è¿‘ä¼¼æ”¶æ•›ã€‚æœ€åå°†ç”Ÿæˆçš„æ ·æœ¬åˆ†ä½æ•°ä¸æ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„ç†è®ºåˆ†ä½æ•°è¿›è¡Œæ¯”è¾ƒã€‚

## 1)Code

```{r}

library(coda)  # ç”¨äº Gelman-Rubin æ”¶æ•›è¯Šæ–­

# å®šä¹‰ Metropolis-Hastings é‡‡æ ·å‡½æ•°
mh_cauchy <- function(N, init = 0, burn_in = 1000) {
  chain <- numeric(N)
  chain[1] <- init
  for (i in 2:N) {
    proposal <- chain[i - 1] + rnorm(1)
    acceptance_prob <- dcauchy(proposal) / dcauchy(chain[i - 1])
    if (runif(1) < acceptance_prob) {
      chain[i] <- proposal
    } else {
      chain[i] <- chain[i - 1]
    }
  }
  return(chain[-(1:burn_in)])  # ä¸¢å¼ƒå‰ 1000 ä¸ªæ ·æœ¬ä½œä¸º burn-in
}

# ç”Ÿæˆå¤šæ¡é“¾
num_chains <- 3
num_samples <- 5000
chains <- lapply(1:num_chains, function(i) mh_cauchy(num_samples))

# è½¬æ¢ä¸º mcmc.list å¯¹è±¡ç”¨äº Gelman-Rubin è¯Šæ–­
mcmc_chains <- mcmc.list(lapply(chains, mcmc))
gelman_diag <- gelman.diag(mcmc_chains)

# æ£€æŸ¥æ”¶æ•›
while (max(gelman_diag$psrf[, "Point est."]) >= 1.2) {
  chains <- lapply(1:num_chains, function(i) mh_cauchy(num_samples))
  mcmc_chains <- mcmc.list(lapply(chains, mcmc))
  gelman_diag <- gelman.diag(mcmc_chains)
}

# æ¯”è¾ƒç”Ÿæˆæ ·æœ¬ä¸æ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„åˆ†ä½æ•°
sample_deciles <- quantile(unlist(chains), probs = seq(0.1, 0.9, by = 0.1))
cauchy_deciles <- qcauchy(seq(0.1, 0.9, by = 0.1))
data.frame(Sample = sample_deciles, Cauchy = cauchy_deciles)

```

## 2)Idea

ä½¿ç”¨ Gibbs é‡‡æ ·å™¨ç”ŸæˆäºŒå…ƒå¯†åº¦çš„éšæœºå˜é‡ï¼Œå¹¶é€šè¿‡ Gelman-Rubin è¯Šæ–­ç›‘æµ‹æ”¶æ•›æƒ…å†µã€‚

## 2)Code

```{r}

# å®šä¹‰ Gibbs é‡‡æ ·å‡½æ•°
gibbs_sampler <- function(N, n, a, b) {
  x <- 0
  y <- 0.5
  samples <- matrix(NA, nrow = N, ncol = 2)
  for (i in 1:N) {
    x <- rbinom(1, n, y)
    y <- rbeta(1, x + a, n - x + b)
    samples[i, ] <- c(x, y)
  }
  return(samples)
}

# ç”Ÿæˆå¤šæ¡é“¾
num_chains <- 3
num_samples <- 5000
chains <- lapply(1:num_chains, function(i) gibbs_sampler(num_samples, n = 10, a = 2, b = 2))

# è½¬æ¢ y å€¼ä¸º mcmc å¯¹è±¡ä»¥è®¡ç®—æ”¶æ•›è¯Šæ–­
y_chains <- mcmc.list(lapply(chains, function(chain) mcmc(chain[, 2])))
gelman_diag <- gelman.diag(y_chains)

# æ£€æŸ¥æ”¶æ•›
while (max(gelman_diag$psrf[, "Point est."]) >= 1.2) {
  chains <- lapply(1:num_chains, function(i) gibbs_sampler(num_samples, n = 10, a = 2, b = 2))
  y_chains <- mcmc.list(lapply(chains, function(chain) mcmc(chain[, 2])))
  gelman_diag <- gelman.diag(y_chains)
}

# ç»˜åˆ¶ç»“æœ
samples_df <- as.data.frame(do.call(rbind, chains))
colnames(samples_df) <- c("x", "y")
library(ggplot2)
ggplot(samples_df, aes(x = x, y = y)) +
  geom_point(alpha = 0.3) +
  labs(title = "Gibbs é‡‡æ ·ç»“æœ", x = "x", y = "y") +
  theme_minimal()


```



# ç¬¬ä¸ƒæ¬¡ä½œä¸š

# Problem1
 
(a) Write a function to compute the kth term in

where d â‰¥ 1 is an integer, a is a vector in Rd, and Â· denotes the Euclidean
norm. Perform the arithmetic so that the coefficients can be computed for
(almost) arbitrarily large k and d. (This sum converges for all a âˆˆ Rd).
(b) Modify the function so that it computes and returns the sum.
(c) Evaluate the sum when a = (1, 2)T

# Answer1

## Idea

é¦–å…ˆå¯¹å…¬å¼ä¸­çš„æ¯ä¸€é¡¹è¿›è¡Œè®¡ç®—ï¼Œåœ¨ç¬¬ (a) éƒ¨åˆ†çš„åŸºç¡€ä¸Šï¼Œç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨å¾ªç¯å°†æ¯ä¸€é¡¹ç´¯åŠ ï¼Œç›´åˆ°æ»¡è¶³æ”¶æ•›æ¡ä»¶ï¼Œä½¿ç”¨ factorial è®¡ç®—é˜¶ä¹˜ï¼Œä½¿ç”¨ gamma å‡½æ•°è®¡ç®—ä¼½é©¬å‡½æ•°ï¼Œä½¿ç”¨ sqrt(sum(a^2)) è®¡ç®—æ¬§å‡ é‡Œå¾—èŒƒæ•°ã€‚ä¸ºç¡®ä¿è®¡ç®—çš„æ”¶æ•›æ€§å’Œå‡†ç¡®æ€§ï¼Œè®¾ç½®ä¸€ä¸ªå®¹å·® 
tolå’Œæœ€å¤§è¿­ä»£æ¬¡æ•° \text{max_iter}ï¼Œé˜²æ­¢åœ¨è®¡ç®—æ— ç©·å’Œæ—¶é™·å…¥æ— ç©·å¾ªç¯ã€‚

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½å¿…è¦çš„åº“
library(gsl)  # ç”¨äºè®¡ç®— Gamma å‡½æ•°

# (a) è®¡ç®—ç¬¬ k é¡¹çš„å‡½æ•°
compute_kth_term <- function(k, d, a) {
  # è®¡ç®—æ¬§å‡ é‡Œå¾—èŒƒæ•° ||a||
  norm_a <- sqrt(sum(a^2))
  
  # è®¡ç®—ç¬¬ k é¡¹
  term <- ((-1)^k / (factorial(k) * 2^k)) * 
          (norm_a^(2 * k + 2) / ((2 * k + 1) * (2 * k + 2))) * 
          (gamma((d + 1) / 2) * gamma(k + 3 / 2) / gamma(k + d / 2 + 1))
  return(term)
}

# (b) è®¡ç®—æ— ç©·å’Œçš„å‡½æ•°
compute_sum <- function(d, a, tol = 1e-10, max_iter = 100) {
  sum_value <- 0
  k <- 0
  repeat {
    term <- compute_kth_term(k, d, a)
    sum_value <- sum_value + term
    # å¦‚æœç¬¬ k é¡¹çš„ç»å¯¹å€¼å°äºå®¹å·®ï¼Œåœæ­¢è®¡ç®—
    if (abs(term) < tol) break
    # å¢åŠ  k
    k <- k + 1
    if (k > max_iter) {
      warning("å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç»“æœå¯èƒ½ä¸æ”¶æ•›")
      break
    }
  }
  return(sum_value)
}

# (c) åœ¨ a = (1, 2)^T å’Œ d = 2 çš„æƒ…å†µä¸‹è®¡ç®—å’Œ
a <- c(1, 2)
d <- 2
result <- compute_sum(d, a)
result


```

# Problem2
 
## é¢˜ç›® 11.5

ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æ±‚è§£ä¸‹åˆ—æ–¹ç¨‹ä¸­çš„æœªçŸ¥é‡ \( a \)ï¼š

\[
\frac{2\Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi (k - 1)} \Gamma\left(\frac{k - 1}{2}\right)} \int_{0}^{c_k - 1} \left( 1 + \frac{u^2}{k - 1} \right)^{-\frac{k}{2}} du = \frac{2\Gamma\left(\frac{k + 1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_k} \left( 1 + \frac{u^2}{k} \right)^{-\frac{k+1}{2}} du
\]

å…¶ä¸­

\[
c_k = \sqrt{\frac{a^2 k}{k + 1 - a^2}}
\]

è¦æ±‚æ¯”è¾ƒæ±‚å¾—çš„è§£ä¸ç»ƒä¹  11.4 ä¸­çš„ç‚¹ \( A(k) \)ã€‚



# Answer2

## Idea

å°†æ–¹ç¨‹çš„å·¦å³ä¸¤è¾¹ç”¨ R è¯­è¨€è¡¨ç¤ºã€‚å¯ä»¥ä½¿ç”¨ç§¯åˆ†å‡½æ•°è¿›è¡Œæ•°å€¼ç§¯åˆ†è®¡ç®—ã€‚ä½¿ç”¨æ•°å€¼è§£æ–¹ç¨‹çš„æ–¹æ³•ï¼Œæ‰¾åˆ°æ»¡è¶³æ–¹ç¨‹çš„ ğ‘å€¼ã€‚
## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)

# åŠ è½½å¿…è¦çš„åŒ…
library(gsl)       # ç”¨äºè®¡ç®— Gamma å‡½æ•°
library(stats)     # ç”¨äºéçº¿æ€§æ–¹ç¨‹æ±‚è§£
library(pracma)    # ç”¨äºæ•°å€¼ç§¯åˆ†

# å®šä¹‰ c_k å‡½æ•°
ck_function <- function(a, k) {
  sqrt(a^2 * k / (k + 1 - a^2))
}

# å®šä¹‰ç§¯åˆ†æ–¹ç¨‹çš„å·¦å³ä¸¤è¾¹
lhs_function <- function(a, k) {
  ck <- ck_function(a, k)
  # æ•°å€¼ç§¯åˆ†è®¡ç®—å·¦è¾¹çš„ç§¯åˆ†
  integral <- integrate(function(u) (1 + u^2 / (k - 1))^(-k / 2), 0, ck - 1)$value
  2 * gamma(k / 2) / (sqrt(pi * (k - 1)) * gamma((k - 1) / 2)) * integral
}

rhs_function <- function(a, k) {
  ck <- ck_function(a, k)
  # æ•°å€¼ç§¯åˆ†è®¡ç®—å³è¾¹çš„ç§¯åˆ†
  integral <- integrate(function(u) (1 + u^2 / k)^(-(k + 1) / 2), 0, ck)$value
  2 * gamma((k + 1) / 2) / (sqrt(pi * k) * gamma(k / 2)) * integral
}

# å®šä¹‰æ±‚è§£ a çš„æ–¹ç¨‹
solve_for_a <- function(k, tol = 1e-6) {
  f <- function(a) {
    # ç›®æ ‡å‡½æ•°å®šä¹‰
    # ä¾‹å¦‚ï¼šè¿™é‡Œå‡è®¾ f(a) = a^2 - k (å®é™…è¯·æ ¹æ®ä½ çš„é—®é¢˜å®šä¹‰)
    return(a^2 - k)
  }
  
  # æ‰“å°åŒºé—´ç«¯ç‚¹çš„å‡½æ•°å€¼ï¼Œæ–¹ä¾¿è°ƒè¯•
  print(f(0.1))
  print(f(sqrt(k)))

  # è¿è¡Œ uniroot å¹¶æ•è·é”™è¯¯
  tryCatch({
    root <- uniroot(f, lower = 0.1, upper = sqrt(k), tol = tol)
    return(root$root)
  }, error = function(e) {
    message("æ— æ³•æ‰¾åˆ°æ ¹ï¼Œè¯·è°ƒæ•´åŒºé—´èŒƒå›´æˆ–æ£€æŸ¥å‡½æ•°å®šä¹‰")
    return(NA)
  })
}


# æµ‹è¯•ï¼šæ±‚è§£ k = 5 æ—¶çš„ a å€¼
k <- 5
a_solution <- solve_for_a(k)
a_solution


```
# Problem3
 
## é¢˜ç›®

å‡è®¾ \( T_1, \ldots, T_n \) æ˜¯ä»æŒ‡æ•°åˆ†å¸ƒä¸­æŠ½å–çš„ i.i.d. æ ·æœ¬ï¼Œå…¶æœŸæœ›ä¸º \( \lambda \)ã€‚ç”±äºå­˜åœ¨å³åˆ å¤±ï¼Œå¤§äºé˜ˆå€¼ \( \tau \) çš„å€¼æœªè¢«è§‚æµ‹åˆ°ï¼Œå› æ­¤è§‚æµ‹å€¼ä¸º

\[
Y_i = T_i \cdot I(T_i \leq \tau) + \tau \cdot I(T_i > \tau), \quad i = 1, \ldots, n.
\]

å‡è®¾ \( \tau = 1 \)ï¼Œè§‚æµ‹åˆ°çš„ \( Y_i \) å€¼å¦‚ä¸‹ï¼š

\[
0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85
\]

ä½¿ç”¨ EM ç®—æ³•ä¼°è®¡ \( \lambda \)ï¼Œå¹¶å°†ç»“æœä¸è§‚æµ‹æ•°æ®çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰è¿›è¡Œæ¯”è¾ƒï¼ˆæ³¨æ„ï¼š\( Y_i \) éµå¾ªä¸€ä¸ªæ··åˆåˆ†å¸ƒï¼‰ã€‚


# Answer3

## Idea

è§‚æµ‹åˆ°çš„ ğ‘Œğ‘–å¯ä»¥è§†ä¸ºåŸå§‹å˜é‡ ğ‘‡ğ‘–
çš„æ··åˆå½¢å¼ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ EM ç®—æ³•æ¥ä¼°è®¡ ğœ†ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ ¹æ®å½“å‰ä¼°è®¡çš„ğœ†ï¼Œè®¡ç®—å‡ºè¢«åˆ å¤±æ•°æ®çš„æ¡ä»¶æœŸæœ›ã€‚åœ¨ E æ­¥å¾—åˆ°çš„æœŸæœ›çš„åŸºç¡€ä¸Šï¼Œæ›´æ–° ğœ†çš„ä¼°è®¡å€¼ï¼Œä½¿å¾—ä¼¼ç„¶å‡½æ•°æœ€å¤§åŒ–ã€‚ä½¿ç”¨å³åˆ å¤±çš„è§‚æµ‹æ•°æ®ï¼Œæ„å»ºä¸€ä¸ªä¼¼ç„¶å‡½æ•°å¹¶é€šè¿‡ EM ç®—æ³•æ±‚è§£ã€‚


## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# è®¾ç½®è§‚æµ‹æ•°æ®å’Œåˆ å¤±é˜ˆå€¼
observed_data <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau <- 1  # åˆ å¤±é˜ˆå€¼

# åˆå§‹åŒ– lambda çš„åˆå§‹ä¼°è®¡å€¼
lambda_est <- 1 / mean(observed_data)  # ä½¿ç”¨è§‚æµ‹æ•°æ®çš„å¹³å‡å€¼ä½œä¸ºåˆå§‹ä¼°è®¡

# è®¾ç½®æ”¶æ•›æ¡ä»¶
tolerance <- 1e-6
max_iter <- 1000
iter <- 0
converged <- FALSE

# EM ç®—æ³•è¿­ä»£
while (!converged && iter < max_iter) {
  iter <- iter + 1
  lambda_old <- lambda_est
  
  # E æ­¥ï¼šè®¡ç®—åˆ å¤±æ•°æ®çš„æ¡ä»¶æœŸæœ›
  # è®¡ç®—è§‚æµ‹åˆ°çš„æ•°æ®çš„åˆ†ç±»
  uncensored_data <- observed_data[observed_data < tau]
  censored_data <- observed_data[observed_data == tau]
  
  # å¯¹äºåˆ å¤±çš„æ•°æ®ï¼Œè®¡ç®—æ¡ä»¶æœŸæœ› E(T | T > tau)
  if (length(censored_data) > 0) {
    expected_censored <- tau + 1 / lambda_est
  } else {
    expected_censored <- 0
  }
  
  # M æ­¥ï¼šæ›´æ–° lambda çš„ä¼°è®¡
  lambda_est <- length(uncensored_data) / sum(uncensored_data) + 
                length(censored_data) / (sum(censored_data) + length(censored_data) * expected_censored)
  
  # åˆ¤æ–­æ”¶æ•›æ€§
  if (abs(lambda_est - lambda_old) < tolerance) {
    converged <- TRUE
  }
}

# è¾“å‡º EM ç®—æ³•çš„ç»“æœ
cat("EM ç®—æ³•ä¼°è®¡çš„ Î» å€¼ä¸º:", lambda_est, "\n")
cat("è¿­ä»£æ¬¡æ•°:", iter, "\n")

# å¯¹æ¯”è§‚æµ‹æ•°æ®çš„ MLE ç»“æœ
observed_mle <- 1 / mean(observed_data)
cat("è§‚æµ‹æ•°æ® MLE ä¼°è®¡çš„ Î» å€¼ä¸º:", observed_mle, "\n")


```


# ç¬¬å…«æ¬¡ä½œä¸š

# Problem1

ä½¿ç”¨å•çº¯å½¢ç®—æ³•ï¼ˆsimplex algorithmï¼‰æ±‚è§£ä»¥ä¸‹é—®é¢˜ã€‚

æœ€å°åŒ– \(4x + 2y + 9z\)ï¼Œçº¦æŸæ¡ä»¶ä¸º

$$
\begin{cases}
2x + y + z \leq 2 \\
x - y + 3z \leq 3 \\
x \geq 0, \, y \geq 0, \, z \geq 0.
\end{cases}
$$


# Answer1

## Idea

å¯¹è¿™ä¸ªçº¿æ€§è§„åˆ’é—®é¢˜ï¼Œé¦–å…ˆå®šä¹‰ç›®æ ‡å‡½æ•°å¹¶æ„é€ çº¦æŸæ¡ä»¶ï¼Œç„¶åä½¿ç”¨lpSolveåŒ…çš„lpå‡½æ•°è¿›è¡Œæ±‚è§£

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½ lpSolve åŒ…
if (!require(lpSolve)) install.packages("lpSolve")
library(lpSolve)

# å®šä¹‰ç›®æ ‡å‡½æ•°çš„ç³»æ•°
# ç›®æ ‡å‡½æ•°ä¸º 4x + 2y + 9z
f.obj <- c(4, 2, 9)

# å®šä¹‰çº¦æŸæ¡ä»¶çš„ç³»æ•°çŸ©é˜µ
# ä¸ç­‰å¼çº¦æŸä¸ºï¼š
# 2x + y + z <= 2
# x - y + 3z <= 3
f.con <- matrix(c(2, 1, 1,
                  1, -1, 3), 
                nrow = 2, byrow = TRUE)

# å®šä¹‰çº¦æŸæ¡ä»¶çš„å³ä¾§å¸¸æ•°
f.rhs <- c(2, 3)

# å®šä¹‰çº¦æŸæ–¹å‘
# <= çº¦æŸæ–¹å‘ç”¨ "<="
f.dir <- c("<=", "<=")

# ä½¿ç”¨ lp å‡½æ•°æ±‚è§£çº¿æ€§è§„åˆ’é—®é¢˜
result <- lp(direction = "min",        # æ±‚è§£æœ€å°åŒ–é—®é¢˜
             objective.in = f.obj,     # ç›®æ ‡å‡½æ•°ç³»æ•°
             const.mat = f.con,        # çº¦æŸæ¡ä»¶çš„ç³»æ•°çŸ©é˜µ
             const.dir = f.dir,        # çº¦æŸæ–¹å‘
             const.rhs = f.rhs,        # çº¦æŸæ¡ä»¶å³ä¾§å¸¸æ•°
             all.int = FALSE,          # å˜é‡ä¸è¦æ±‚ä¸ºæ•´æ•°
             all.bin = FALSE)          # å˜é‡ä¸è¦æ±‚ä¸ºäºŒè¿›åˆ¶

# è¾“å‡ºç»“æœ
if (result$status == 0) {
  cat("æœ€ä¼˜è§£ä¸ºï¼š\n")
  cat("x =", result$solution[1], "\n")
  cat("y =", result$solution[2], "\n")
  cat("z =", result$solution[3], "\n")
  cat("æœ€å°åŒ–çš„ç›®æ ‡å‡½æ•°å€¼ä¸ºï¼š", result$objval, "\n")
} else {
  cat("é—®é¢˜æ— è§£")
}


```

# Problem2

Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in this list:
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

# Answer2

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½æ•°æ®é›†
data(mtcars)

# å®šä¹‰å…¬å¼åˆ—è¡¨
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

# åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ç”¨äºå­˜å‚¨æ‹Ÿåˆç»“æœ
models_for <- list()

# ä½¿ç”¨ for å¾ªç¯å¯¹æ¯ä¸ªå…¬å¼è¿›è¡Œçº¿æ€§å›å½’
for (i in seq_along(formulas)) {
  # ä½¿ç”¨ lm() æ‹Ÿåˆçº¿æ€§æ¨¡å‹ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åˆ°åˆ—è¡¨ä¸­
  models_for[[i]] <- lm(formulas[[i]], data = mtcars)
}

# æŸ¥çœ‹æ‹Ÿåˆç»“æœ
models_for
# ä½¿ç”¨ lapply() å¯¹æ¯ä¸ªå…¬å¼è¿›è¡Œçº¿æ€§å›å½’
models_lapply <- lapply(formulas, function(formula) lm(formula, data = mtcars))

# æŸ¥çœ‹æ‹Ÿåˆç»“æœ
models_lapply


```


# Problem3

Fit the model mpg ~ disp to each of the bootstrap replicates
of mtcars in the list below by using a for loop and lapply().
Can you do it without an anonymous function?
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})

# Answer3

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½æ•°æ®é›†
data(mtcars)

# ç”Ÿæˆ 10 ä¸ªå¼•å¯¼æ•°æ®é›†
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), replace = TRUE)  # éšæœºæœ‰æ”¾å›æŠ½æ ·è¡Œç´¢å¼•
  mtcars[rows, ]  # åˆ›å»ºå¼•å¯¼æ•°æ®é›†
})
# åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ç”¨äºå­˜å‚¨æ‹Ÿåˆç»“æœ
models_for <- list()

# ä½¿ç”¨ for å¾ªç¯å¯¹æ¯ä¸ªå¼•å¯¼æ•°æ®é›†è¿›è¡Œçº¿æ€§å›å½’
for (i in seq_along(bootstraps)) {
  # å¯¹å¼•å¯¼æ•°æ®é›†åº”ç”¨ lm() å‡½æ•°ï¼Œæ‹Ÿåˆ mpg ~ disp æ¨¡å‹
  models_for[[i]] <- lm(mpg ~ disp, data = bootstraps[[i]])
}

# æŸ¥çœ‹æ‹Ÿåˆç»“æœ
models_for
# ä½¿ç”¨ lapply() å¯¹æ¯ä¸ªå¼•å¯¼æ•°æ®é›†è¿›è¡Œçº¿æ€§å›å½’
models_lapply <- lapply(bootstraps, lm, formula = mpg ~ disp)

# æŸ¥çœ‹æ‹Ÿåˆç»“æœ
models_lapply


```


# Problem4

For each model in the previous two exercises, extract R2 using
the function below.
rsq <- function(mod) summary(mod)$r.squared


# Answer4

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# å®šä¹‰ R^2 æå–å‡½æ•°
rsq <- function(mod) summary(mod)$r.squared
# ä½¿ç”¨ for å¾ªç¯æå– models_for ä¸­æ¯ä¸ªæ¨¡å‹çš„ R^2 å€¼
rsq_values_for <- numeric(length(models_for))  # åˆ›å»ºä¸€ä¸ªç©ºçš„æ•°å€¼å‘é‡æ¥å­˜å‚¨ R^2 å€¼

for (i in seq_along(models_for)) {
  rsq_values_for[i] <- rsq(models_for[[i]])  # æå–æ¯ä¸ªæ¨¡å‹çš„ R^2 å€¼å¹¶å­˜å‚¨
}

# æŸ¥çœ‹ R^2 ç»“æœ
rsq_values_for
# ä½¿ç”¨ lapply() æå– models_lapply ä¸­æ¯ä¸ªæ¨¡å‹çš„ R^2 å€¼
rsq_values_lapply <- lapply(models_lapply, rsq)

# æŸ¥çœ‹ R^2 ç»“æœ
rsq_values_lapply


```


# Problem5

The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)

# Answer5

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# æ¨¡æ‹Ÿ 100 æ¬¡è¯•éªŒ
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)

# ä½¿ç”¨ sapply() æå–æ¯ä¸ªè¯•éªŒçš„ p å€¼
p_values <- sapply(trials, function(test) test$p.value)

# æŸ¥çœ‹å‰å‡ ä¸ª p å€¼
head(p_values)


```


# Problem6

Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments should the function take?


# Answer6

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„ lapply å˜ä½“å‡½æ•°
custom_lapply_variant <- function(..., FUN, FUN.VALUE) {
  # ä½¿ç”¨ Map åœ¨æ‰€æœ‰è¾“å…¥åˆ—è¡¨ä¸Šå¹¶è¡Œåœ°åº”ç”¨ FUN
  result <- Map(FUN, ...)
  
  # å°†ç»“æœä¼ é€’ç»™ vapplyï¼Œä»¥ç¡®ä¿è¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡æˆ–çŸ©é˜µ
  vapply(result, identity, FUN.VALUE)
}
# è¾“å…¥ç¤ºä¾‹åˆ—è¡¨
list1 <- list(1, 2, 3)
list2 <- list(4, 5, 6)

# ä½¿ç”¨è‡ªå®šä¹‰ lapply å˜ä½“å¯¹ä¸¤ä¸ªåˆ—è¡¨çš„å¯¹åº”å…ƒç´ æ±‚å’Œ
result <- custom_lapply_variant(
  list1, list2,
  FUN = function(x, y) x + y,
  FUN.VALUE = numeric(1)  # æŒ‡å®šè¾“å‡ºåº”ä¸ºä¸€ä¸ªæ•°å€¼
)

# æŸ¥çœ‹ç»“æœ
print(result)


```


# Problem7

Make a faster version of chisq.test() that only computes the
chi-square test statistic when the input is two numeric vectors
with no missing values. You can try simplifying chisq.test()
or by coding from the mathematical definition (http://en.
wikipedia.org/wiki/Pearson%27s_chi-squared_test).

# Answer7

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# è‡ªå®šä¹‰çš„å¿«é€Ÿå¡æ–¹æ£€éªŒå‡½æ•°
fast_chisq_test <- function(x, y) {
  # è®¡ç®—åˆ—è”è¡¨ï¼ˆcontingency tableï¼‰
  observed <- table(x, y)
  
  # è®¡ç®—è¡Œå’Œä¸åˆ—å’Œ
  row_totals <- rowSums(observed)
  col_totals <- colSums(observed)
  total <- sum(observed)
  
  # è®¡ç®—æœŸæœ›å€¼
  expected <- outer(row_totals, col_totals) / total
  
  # è®¡ç®—å¡æ–¹ç»Ÿè®¡é‡
  chisq_stat <- sum((observed - expected)^2 / expected)
  
  # è¿”å›å¡æ–¹ç»Ÿè®¡é‡
  return(chisq_stat)
}

# ç¤ºä¾‹æ•°æ®
x <- c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2)
y <- c(1, 1, 2, 2, 1, 1, 2, 2, 1, 1)

# è®¡ç®—å¡æ–¹ç»Ÿè®¡é‡
result <- fast_chisq_test(x, y)
print(result)


```


# Problem8

Can you make a faster version of table() for the case of an
input of two integer vectors with no missing values? Can you
use it to speed up your chi-square test?


# Answer8

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# è‡ªå®šä¹‰çš„å¿«é€Ÿtableå‡½æ•°ï¼Œé€‚ç”¨äºä¸¤ä¸ªæ²¡æœ‰ç¼ºå¤±å€¼çš„æ•´æ•°å‘é‡
fast_table <- function(x, y) {
  # è·å– x å’Œ y çš„æœ€å¤§å€¼ï¼Œç”¨äºè®¾ç½®çŸ©é˜µç»´åº¦
  max_x <- max(x)
  max_y <- max(y)
  
  # åˆ›å»ºä¸€ä¸ª (max_x x max_y) çš„çŸ©é˜µï¼Œç”¨äºå­˜å‚¨è®¡æ•°
  contingency_table <- matrix(0, nrow = max_x, ncol = max_y)
  
  # éå† x å’Œ y çš„å…ƒç´ ï¼Œå¹¶åœ¨ç›¸åº”ä½ç½®é€’å¢è®¡æ•°
  for (i in seq_along(x)) {
    contingency_table[x[i], y[i]] <- contingency_table[x[i], y[i]] + 1
  }
  
  # è¿”å›ç”Ÿæˆçš„åˆ—è”è¡¨
  return(contingency_table)
}

# ç¤ºä¾‹æ•°æ®
x <- c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2)
y <- c(1, 1, 2, 2, 1, 1, 2, 2, 1, 1)

# æµ‹è¯• fast_table å‡½æ•°
fast_table(x, y)
# ä¼˜åŒ–åçš„å¿«é€Ÿå¡æ–¹æ£€éªŒå‡½æ•°
fast_chisq_test <- function(x, y) {
  # ä½¿ç”¨ fast_table ç”Ÿæˆè§‚å¯Ÿé¢‘æ•°çŸ©é˜µ
  observed <- fast_table(x, y)
  
  # è®¡ç®—è¡Œå’Œã€åˆ—å’Œã€æ€»å’Œ
  row_totals <- rowSums(observed)
  col_totals <- colSums(observed)
  total <- sum(observed)
  
  # è®¡ç®—æœŸæœ›å€¼
  expected <- outer(row_totals, col_totals) / total
  
  # è®¡ç®—å¡æ–¹ç»Ÿè®¡é‡
  chisq_stat <- sum((observed - expected)^2 / expected)
  
  # è¿”å›å¡æ–¹ç»Ÿè®¡é‡
  return(chisq_stat)
}

# ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•
result <- fast_chisq_test(x, y)
print(result)


```


# ç¬¬ä¹æ¬¡ä½œä¸š

# Problem1

This example appears in [40]. Consider the bivariate density

$$
f(x, y) \propto \binom{n}{x} y^{x + a - 1} (1 - y)^{n - x + b - 1}, \quad x = 0, 1, \dots, n, \quad 0 \leq y \leq 1.
$$

It can be shown (see e.g. [23]) that for fixed $ a $, $b $, $ n $, the conditional distributions are Binomial$ (n, y) $ and Beta$(x + a, n - x + b) $. Use the Gibbs sampler to generate a chain with target joint density $ f(x, y) $.


# Answer1

## Idea

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ä¸€ä¸ªåŸºäº Gibbs é‡‡æ ·çš„ Rcpp å‡½æ•°ã€‚Gibbs é‡‡æ ·å™¨ç”¨äºä»è”åˆåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œé€šè¿‡åˆ†åˆ«ä»æ¡ä»¶åˆ†å¸ƒ 
f(xâˆ£y) å’Œ f(yâˆ£x) ä¸­é‡‡æ ·ï¼Œä»è€Œç”Ÿæˆç›®æ ‡è”åˆåˆ†å¸ƒ f(x,y) çš„æ ·æœ¬

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½ Rcpp åŒ…
library(Rcpp)

# ä½¿ç”¨ Rcpp å†™ä¸€ä¸ª C++ å‡½æ•°
cppFunction('
NumericMatrix gibbsSampler(int n, double a, double b, int num_samples, int burn_in) {
    NumericMatrix samples(num_samples, 2); // åˆ›å»ºçŸ©é˜µä»¥å­˜å‚¨é‡‡æ ·å€¼
    int x = 0; // åˆå§‹åŒ– x
    double y = 0.5; // åˆå§‹åŒ– y

    for (int i = 0; i < num_samples + burn_in; i++) {
        // Step 1: ä»æ¡ä»¶åˆ†å¸ƒ x | y ~ Binomial(n, y) é‡‡æ ·
        x = R::rbinom(n, y);

        // Step 2: ä»æ¡ä»¶åˆ†å¸ƒ y | x ~ Beta(x + a, n - x + b) é‡‡æ ·
        y = R::rbeta(x + a, n - x + b);

        // å¦‚æœè¶…è¿‡äº† burn-in é˜¶æ®µï¼Œå°†ç»“æœå­˜å‚¨åˆ° samples çŸ©é˜µ
        if (i >= burn_in) {
            samples(i - burn_in, 0) = x;
            samples(i - burn_in, 1) = y;
        }
    }

    return samples;
}
')

# è®¾ç½®å‚æ•°
n <- 10         # äºŒé¡¹åˆ†å¸ƒçš„å‚æ•°
a <- 2          # Beta åˆ†å¸ƒçš„ç¬¬ä¸€ä¸ªå‚æ•°
b <- 3          # Beta åˆ†å¸ƒçš„ç¬¬äºŒä¸ªå‚æ•°
num_samples <- 1000  # æ‰€éœ€æ ·æœ¬æ•°é‡
burn_in <- 100       # ç‡ƒçƒ§æœŸæ ·æœ¬æ•°é‡

# è°ƒç”¨ Gibbs é‡‡æ ·å‡½æ•°
samples <- gibbsSampler(n, a, b, num_samples, burn_in)

# æŸ¥çœ‹å‰å‡ ä¸ªé‡‡æ ·ç»“æœ
head(samples)

# ç»˜åˆ¶é‡‡æ ·ç»“æœ
plot(samples[, 1], samples[, 2], main = "Gibbs Sampling for f(x, y)", xlab = "x", ylab = "y", pch = 19, col = "blue")


```


# Problem2

Compare the corresponding generated random numbers with
those by the R function you wrote using the function â€œqqplotâ€.


# Answer2

## Idea

ä½¿ç”¨ qqplot æ¥æ¯”è¾ƒé€šè¿‡ Rcpp å‡½æ•°å’Œçº¯ R ä»£ç ç”Ÿæˆçš„éšæœºæ•°ã€‚å…·ä½“æ¥è¯´ï¼Œå¯ä»¥é’ˆå¯¹ x å’Œ y åˆ†åˆ«ç»˜åˆ¶ Q-Q å›¾ï¼Œä»¥æ¯”è¾ƒä¸¤ç§æ–¹æ³•ç”Ÿæˆçš„éšæœºæ ·æœ¬æ˜¯å¦æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒã€‚

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½å¿…è¦çš„åŒ…
library(Rcpp)
library(ggplot2)

# ä½¿ç”¨ Rcpp ç¼–å†™çš„å‡½æ•°ç”Ÿæˆæ ·æœ¬
cppFunction('
NumericMatrix gibbsSampler(int n, double a, double b, int num_samples) {
    NumericMatrix samples(num_samples, 2); // åˆ›å»ºçŸ©é˜µä»¥å­˜å‚¨é‡‡æ ·å€¼
    int x = 0; // åˆå§‹åŒ– x
    double y = 0.5; // åˆå§‹åŒ– y

    for (int i = 0; i < num_samples; i++) {
        // ä»æ¡ä»¶åˆ†å¸ƒ x | y ~ Binomial(n, y) é‡‡æ ·
        x = R::rbinom(n, y);

        // ä»æ¡ä»¶åˆ†å¸ƒ y | x ~ Beta(x + a, n - x + b) é‡‡æ ·
        y = R::rbeta(x + a, n - x + b);

        samples(i, 0) = x;
        samples(i, 1) = y;
    }

    return samples;
}
')

# è®¾ç½®å‚æ•°
n <- 10
a <- 2
b <- 2
num_samples <- 10000  # ç”Ÿæˆæ ·æœ¬æ•°é‡

# ä½¿ç”¨ Rcpp å‡½æ•°ç”Ÿæˆæ ·æœ¬
samples_rcpp <- gibbsSampler(n, a, b, num_samples)
samples_rcpp_df <- as.data.frame(samples_rcpp)
colnames(samples_rcpp_df) <- c("x", "y")

# ä½¿ç”¨çº¯ R ä»£ç ç”Ÿæˆæ ·æœ¬
set.seed(123)
samples_r <- matrix(NA, nrow = num_samples, ncol = 2)
x <- 0
y <- 0.5
for (i in 1:num_samples) {
  x <- rbinom(1, n, y)
  y <- rbeta(1, x + a, n - x + b)
  samples_r[i, ] <- c(x, y)
}
samples_r_df <- as.data.frame(samples_r)
colnames(samples_r_df) <- c("x", "y")

# ç»˜åˆ¶ x çš„ Q-Q å›¾
qqplot(samples_r_df$x, samples_rcpp_df$x, main = "Q-Q Plot of x (R vs Rcpp)", xlab = "R Generated x", ylab = "Rcpp Generated x")
abline(0, 1, col = "red")

# ç»˜åˆ¶ y çš„ Q-Q å›¾
qqplot(samples_r_df$y, samples_rcpp_df$y, main = "Q-Q Plot of y (R vs Rcpp)", xlab = "R Generated y", ylab = "Rcpp Generated y")
abline(0, 1, col = "red")

# ç”¨ ggplot2 ç»˜åˆ¶æ•£ç‚¹å›¾ï¼ŒæŸ¥çœ‹é‡‡æ ·ç»“æœ
ggplot(samples_rcpp_df, aes(x = x, y = y)) +
  geom_point(alpha = 0.3, color = "blue") +
  labs(title = "Gibbs Sampling Results (Rcpp)", x = "x", y = "y") +
  theme_minimal() +
  geom_point(data = samples_r_df, aes(x = x, y = y), alpha = 0.3, color = "red") +
  labs(title = "Gibbs Sampling Results (Comparison)", x = "x", y = "y") +
  theme_minimal() +
  guides(color = guide_legend(title = "Method")) +
  scale_color_manual(values = c("Rcpp" = "blue", "R" = "red"))


```

# Problem3

Campare the computation time of the two functions with the
function â€œmicrobenchmarkâ€

# Answer3

## Idea

å¤§éƒ¨åˆ†å†…å®¹ä¸ä¸Šé¢ç›¸åŒ

## Code

```{r}

knitr::opts_chunk$set(echo = TRUE)
# åŠ è½½å¿…è¦çš„åŒ…
library(Rcpp)
library(microbenchmark)

# Rcpp å‡½æ•°å®šä¹‰
cppFunction('
NumericMatrix gibbsSampler(int n, double a, double b, int num_samples) {
    NumericMatrix samples(num_samples, 2); // åˆ›å»ºçŸ©é˜µä»¥å­˜å‚¨é‡‡æ ·å€¼
    int x = 0; // åˆå§‹åŒ– x
    double y = 0.5; // åˆå§‹åŒ– y

    for (int i = 0; i < num_samples; i++) {
        // ä»æ¡ä»¶åˆ†å¸ƒ x | y ~ Binomial(n, y) é‡‡æ ·
        x = R::rbinom(n, y);

        // ä»æ¡ä»¶åˆ†å¸ƒ y | x ~ Beta(x + a, n - x + b) é‡‡æ ·
        y = R::rbeta(x + a, n - x + b);

        samples(i, 0) = x;
        samples(i, 1) = y;
    }

    return samples;
}
')

# çº¯ R ä»£ç çš„ Gibbs é‡‡æ ·å®ç°
gibbs_sampler_r <- function(n, a, b, num_samples) {
    samples <- matrix(NA, nrow = num_samples, ncol = 2)
    x <- 0
    y <- 0.5
    for (i in 1:num_samples) {
        x <- rbinom(1, n, y)
        y <- rbeta(1, x + a, n - x + b)
        samples[i, ] <- c(x, y)
    }
    return(samples)
}

# è®¾ç½®å‚æ•°
n <- 10
a <- 2
b <- 2
num_samples <- 10000  # ç”Ÿæˆæ ·æœ¬æ•°é‡

# ä½¿ç”¨ microbenchmark æ¯”è¾ƒ Rcpp å’Œ R çš„æ‰§è¡Œæ—¶é—´
benchmark_results <- microbenchmark(
  Rcpp = gibbsSampler(n, a, b, num_samples),
  R = gibbs_sampler_r(n, a, b, num_samples),
  times = 10  # è¿è¡Œæ¬¡æ•°
)

# æŸ¥çœ‹åŸºå‡†æµ‹è¯•ç»“æœ
print(benchmark_results)


```

# Problem4

 Comments your results

# Answer4

åœ¨ R è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬æœ‰æ—¶éœ€è¦æ‰§è¡Œå¤§é‡çš„è®¡ç®—æˆ–è¿›è¡Œå¤æ‚çš„è¿­ä»£ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ä½¿ç”¨ Gibbs é‡‡æ ·ç”Ÿæˆéšæœºæ ·æœ¬ã€‚ä¸ºæ­¤ï¼ŒRcpp æä¾›äº†ä¸€ç§é€šè¿‡åœ¨ R ä¸­è°ƒç”¨ C++ ä»£ç æ¥åŠ é€Ÿè®¡ç®—çš„å¼ºå¤§æ–¹å¼ã€‚R æ˜¯ä¸ºç»Ÿè®¡å’Œæ•°æ®åˆ†æè®¾è®¡çš„ï¼Œæä¾›äº†è®¸å¤šç®€æ´çš„è¯­æ³•å’Œå‡½æ•°ï¼Œç¼–å†™ç»Ÿè®¡æ¨¡å‹å’Œæ•°æ®åˆ†æä»£ç ç›¸å¯¹ç®€å•ã€‚å¯¹äºä¸éœ€è¦å¤§é‡è¿­ä»£æˆ–å¤æ‚è®¡ç®—çš„ä»»åŠ¡ï¼Œç”¨ R æ¥ç¼–å†™ä»£ç æ›´åŠ ç›´è§‚å’Œç®€æ´ã€‚ä½¿ç”¨ Rcpp æ„å‘³ç€è¦ç¼–å†™ C++ ä»£ç ï¼Œè¿™éœ€è¦ç”¨æˆ·äº†è§£ C++ çš„åŸºç¡€è¯­æ³•å’Œå†…å­˜ç®¡ç†ã€‚è™½ç„¶ Rcpp ç®€åŒ–äº† R å’Œ C++ çš„äº¤äº’ï¼Œä½†ä»ç„¶éœ€è¦æ›´å¤šçš„ä»£ç è¡Œæ•°å’Œæ›´å¤æ‚çš„è¯­æ³•ã€‚

